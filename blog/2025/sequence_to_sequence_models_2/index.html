<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Translation using deep neural networks - Transformer (part 2) | </title> <meta name="author" content="Adam Amster"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/moon_avatar.png?ff160fd774b875a0b70dabef23945735"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aamster.github.io/blog/2025/sequence_to_sequence_models_2/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script src="https://cdn.plot.ly/plotly-3.0.1.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js"></script> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.css"> <style>.toc-list-item{list-style:none}</style> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm sticky-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <img src="/assets/img/moon_and_reflection.png" style="width: 50px"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about">about </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-2"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-10"> <div class="post"> <header class="post-header"> <h1 class="post-title">Translation using deep neural networks - Transformer (part 2)</h1> <p class="post-meta"> July 09, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep learning</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   <a href="/blog/tag/machine-translation"> <i class="fa-solid fa-hashtag fa-sm"></i> machine translation</a>   <a href="/blog/tag/sequences"> <i class="fa-solid fa-hashtag fa-sm"></i> sequences</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>In the last article we looked at how attention can be used with recurrent neural networks (RNNs) to generate a sequence from an input sequence. Specifically, we looked at the problem of translating an input sequence from one language into another. We saw that with RNNs without attention, the decoder in a translation model must use a single context vector to compress all the information in the source sequence; with attention, we can attend to different tokens at different decoder timesteps, which enables the model to “pay attention to” different things at different timesteps. This seems intuitively helpful, since if we’re translating a sequence, only certain parts of the sequence are relevant at certain decoding timesteps. Let’s revisit the attention weights example from part 1.</p> <figure id="figure-1" style="text-align: center;"> <div id="attention_weights_plot_container" style="width: 100%"></div> <script>fetch("/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/rnn_attention_weights_0.json").then(t=>t.json()).then(t=>{Plotly.newPlot("attention_weights_plot_container",t.data,t.layout)})["catch"](t=>{console.error("Error loading Plotly data:",t)});</script> </figure> <p>This figure shows the attention weights from each of the predicted tokens on the left to the source tokens on the right. Each generated token on the left attends to each source token on the right, and the sum of weights from each generated token add up to 1. Hover over each line to see the weights between tokens more easily. Only weights &gt; 0.1 are being shown for ease of viewing.</p> <p>At the first timestep, we pay attention to the end of sequence token <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>, which indicates that we need to start the translation, and we output <code class="language-plaintext highlighter-rouge">&lt;s&gt;</code>, the start of sequence token.</p> <p>At the second timestep we pay attention to “But” and look forward 1 timestep to “why” and output the French word for “but,” “Mais.”</p> <p>The transformer builds on the idea of attention and uses attention as feature extraction. It was first introduced in the paper <a class="citation" href="#10.5555/3295222.3295349">(Vaswani et al., 2017)</a>.</p> <p>In <a href="#weaknesses-of-rnn-architectures">Weaknesses of RNN architecture</a> I’ll motivate the need for the transformer architecture. In <a href="#transformer-architecture">Transformer architecture</a> I’ll go more in depth into its architecture. Finally, in <a href="#experiments-and-results">Experiments and results</a> I’ll show how the transformer performance compares to the RNN performance, and as a sub-research question, how the original encoder-decoder transformer architecture compares to GPT-style decoder-only architecture in <a href="#encoder-decoder-vs-decoder-only">Encoder-decoder vs decoder-only</a></p> <p>For this post, it helps if you have read <a href="https://aamster.github.io/blog/2024/sequence_to_sequence_models_1/">the previous post</a> or are already familiar with this topic.</p> <h1 id="weaknesses-of-rnn-architecture">Weaknesses of RNN architecture</h1> <h2 id="sequential-processing">sequential processing</h2> <p>Recall that in the RNN we extract features from the input sequence iteratively.</p> <p>To repeate the example from the previous post:</p> <p>Let’s tokenize the sentence as [“I”, “have”, “socks”, “.”]</p> <p>Let the embedding vector be \(x \in \mathbb{R}^2\)</p> <p>Let the hidden state be \(h \in \mathbb{R}^3\).</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/2024-10-03-sequence_to_sequence_translation/vanilla_rnn.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>The RNN works by iteratively processing each token in the input sequence. We start with an initial “hidden state” defined by <code class="language-plaintext highlighter-rouge">h_t</code>. When we process each token, we use the current hidden state <code class="language-plaintext highlighter-rouge">h_t</code> and the current token <code class="language-plaintext highlighter-rouge">x_t</code> as input to update the hidden state. In this way we can keep track of what we have seen so that the model can encode the meaning of the entire sequence. The hidden state can be seen as an internal representation of the sequence, including token <code class="language-plaintext highlighter-rouge">x_t</code> and is collected in the variable <code class="language-plaintext highlighter-rouge">outs</code>. <code class="language-plaintext highlighter-rouge">outs</code> therefore can be seen as a \(\mathbb{R}^{T\times h}\) encoded representation of the sequence, and <code class="language-plaintext highlighter-rouge">h_t</code> as the final representation of the entire sequence.</p> <p>We had to process each token iteratively in order for an arbitrary token \(x_{t}\) to “know about” a previous token \(x_{t'}\). Iterative processing should be avoided if possible, since we would like to make use of the parallel processing that modern GPUs provide.</p> <p>The transformer does not use iterative processing to encode an input sequence but instead uses an attention mechanism so that in parallel, all tokens can “know about” all other previous tokens.</p> <h2 id="only-a-single-attention-vector">Only a single attention vector</h2> <p>In the RNN with attention, we are only calculating an alignment vector between \(q\) and \(K\) a single time, producing a single measure of similarity, or alignment, between the question \(q\) and the keys \(K\). What if we wanted to align the query with the keys in multiple different ways, for example, by looking for nouns, verbs, adjectives, proper nouns, etc. separately. Currently, using a single measure of similarity, these ideas of similarity are all combined into a single measure, which might make it harder for the model to learn complex patterns.</p> <p>The transformer uses multiple <em>heads</em> of attention to achieve this.</p> <h2 id="only-a-single-layer-of-attention">Only a single layer of attention</h2> <p>In the RNN with attention we stack multiple RNNs together to achieve depth but just calculate attention between the outputs. We’d like to instead add more layers to the attention calculation. We know that in deep learning, depth can allow us to learn more complex patterns.</p> <h1 id="transformer-architecture">Transformer architecture</h1> <p>The paper that first introduced the transformer architecture, <a class="citation" href="#10.5555/3295222.3295349">(Vaswani et al., 2017)</a> is called “Attention is all you need.” From that title we might guess that the transformer makes attention a core component of the architecture. Whereas with the RNN with attention we use attention to align the decoder hidden state with the encoder outputs, but still use sequential recurrent processing using \(h_t = f(h_t, x_t; W)\) to update the hidden state, with the transformer we remove the recurrent hidden state calculation and instead calculate attention as the main computational unit and feature extractor.</p> <p>The transformer has many other components, including attention that already existed at the time, and combines them into a novel architecture. Let’s look at the diagram of the architecture from the paper that introduced the transformer:</p> <figure id="figure-2" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/transformer_architecture.png" alt="transformer architecture" style="width: 60%"> <figcaption>Figure 2: Transformer architecture. Figure from <a class="citation" href="#10.5555/3295222.3295349">(Vaswani et al., 2017)</a> </figcaption> </figure> <p>On the left of the diagram is the encoder, and on the right is the decoder. The encoder encodes the source sentence while the decoder generates the translated sentence using the encoded representation as input. The encoder and decoder are largely the same with a few differences.</p> <p>We’ll look at each component in the diagram and start with the encoder.</p> <h2 id="how-transformer-uses-attention">how transformer uses attention</h2> <p>In the encoder, the RNN uses a recurrent hidden state that takes as input the current token \(x_t\) and the previously seen tokens are encoded using the hidden state \(h_t\). This is a sequential operation since we iterate through each token in the sequence. How can this be replaced with a similar operation without iterating through each token?</p> <p>We’ve seen how attention can be computed between the RNN decoder hidden state vector and the encoder outputs; the decoder hidden state is treated as the query \(q\), and the encoder outputs are treated as the keys \(K\). The transformer adapts attention by generalizing it to an arbitrary number of queries and keys. In the transformer encoder, we calculate attention not just between a single vector and a set of keys, but between a set of query vectors and a set of keys. The set of query vectors \(Q\) is the entire source sequence, and the set of keys is the entire source sequence. The queries and the keys are the same! This is known as <em>self-attention</em>. Let’s look at self-attention in detail.</p> <p>First, though, we need to convert the tokens into numeric vectors as with the RNN, which is the which is the <span style=" border: 4px solid black; background-color: rgb(253, 220, 221); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Input<br> Embedding </span> box in the diagram, and is the same as described in the previous post.</p> <h3 id="self-attention">self-attention</h3> <h4 id="step-1-q-k-v-projection">Step 1: Q, K, V projection</h4> <p>Now, we can start to compute self-attention, which is the <span style=" border: 4px solid black; background-color: rgb(250, 219, 179); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Multi-Head<br> Attention </span> box in the diagram. First, we project the sequence embedding matrix 3 times using 3 different linear maps, \(Q_{proj} \in \mathbb{R}^{D \times D}\), \(K_{proj} \in \mathbb{R}^{D \times D}\), and \(V_{proj} \in \mathbb{R}^{D \times D}\). This results in three new tensors, \(Q\), \(K\), and \(V\), all size \(T \times D\). These are the queries, keys, and values respectively.</p> <p>Importantly, <em>notice that we reuse the same sequence to produce the queries, keys, and values.</em> We do not use different sequences as in the RNN with attention.</p> <figure id="figure2" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/projections.png" alt="projections" style="width: 75%"> <figcaption>Figure 3: Query, keys, and values projections </figcaption> </figure> <h4 id="step-2-attention-weights">Step 2: attention weights</h4> <p>We then compute the attention weights, \(A\). These are calculated the same way as in RNN with attention, using the dot product between a query \(q\) and key \(k\). However, in the transformer there is not just one query (as was the case with the decoder hidden state in the RNN with attention) but \(T\) queries and \(T\) keys. In other words, we treat each token as both a query and key, so that we have \(T\) queries and \(T\) keys each of dimension \(D\). We then calculate similarity using dot-product (aka cosine similarity) between all queries and all keys in parallel using matrix multiplies. This results in a \(T \times T\) matrix including the similarity between every <em>query-key</em> combination.</p> <figure id="figure3" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/attention1.png" alt="attention 1" style="width: 50%"> <figcaption>Figure 4: Query and key similarity </figcaption> </figure> <div class="note-box"> <strong>Note:</strong><br> <p>Note the implications of storing a \(T \times T\) matrix. Here \(T\) is known as the <em>context length</em>. It is the maximum length the model can take as input and hold in memory. The model then requires \(O(T^2)\) memory to store attention weights. This can get expensive as \(T\) gets larger. If the context length is \(1024\) which is small for LLMs, then the \(T \times T\) matrix will store over \(1M\) numbers. Before with the RNN with attention, we never explicitly stored the \(T \times T\) matrix because we iterated through the tokens sequentially and maintained a hidden state. But we said that this was not scalable because it requires sequential iteration. With the transformer we’ve parallelized extracting information from the sequence and so have more efficient compute but at the cost of greater memory needs.</p> </div> <p>As before, we divide each product by \(\sqrt{D}\) as explained <a href="https://aamster.github.io/blog/2024/sequence_to_sequence_models_1/#attention">in the previous post</a>. Finally, we use the \({softmax}\) function for each query across all keys to make positive and normalize to a probability distribution. \({softmax}(A_{ij})\) is defined as:</p> \[\text{softmax}(A_{ij}) = \frac{\exp(A_{ij})}{\sum_{k=1}^{T} \exp(A_{ik})}, \quad i,j = 1,\dots, T.\] <p>This gives us a measure of similarity between each query and key, which is in the range \([0, 1]\) and sums to 1 across keys for a given query.</p> <p>Note that we use the terminology \(q_i\) “attends to” \(k_j\) to denote a strong similarity between \(q_i\) and \(k_j\).</p> <h4 id="step-3-weighting-features">Step 3: weighting features</h4> <p>The matrix \(V\) contains feature vectors of length \(d\) for each of the \(k\) keys. Row \(0\) contains a \(d\)-dimensional vector for key \(0\), row 1 a \(d\)-dimensional vector for key \(1\), etc.</p> <figure id="figure5" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/v.png" alt="V" style="width: 75%"> <figcaption>Figure 5: $$V$$ </figcaption> </figure> <p>We’d like to weight each feature vector by the attention weights by multiplying the attention matrix \(A\) with the feature matrix \(V\).</p> <h4 id="example">example</h4> <p>Let’s look at a concrete example using the sequence “The dog barked” and for simplicity let’s tokenize as <code class="language-plaintext highlighter-rouge">["The", "dog", "barked"]</code>. Let \(D\) be 4.</p> <p>Let <span style=" display:inline-block; width:20px; height:20px; background:rgb(0 149 250) ; vertical-align:middle; "></span> denote a value for a <span style="color: rgb(0 149 250)">query</span> and <span style=" display:inline-block; width:20px; height:20px; background:rgb(255 89 75) ; vertical-align:middle; "></span> denote a value for a <span style="color: rgb(255 89 75)">key</span> and <span style="color: rgb(0 149 250)">\(Q\)</span>, <span style="color: rgb(255 89 75)">\(K\)</span> and <span style="color: rgb(255 89 75)">\(V\)</span> be:</p> <figure id="figure-6" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/qkv_example.png" alt="QKV example" style="width: 50%"> <figcaption>Figure 6: QKV example </figcaption> </figure> <p>\(A\) is:</p> <figure id="figure-7" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/a_example.png" alt="A example" style="width: 50%"> <figcaption>Figure 7: A example </figcaption> </figure> <p>This shows that the token <span style="color: rgb(0 149 250)">“The”</span> paid attention most strongly to <span style="color: rgb(255 89 75)">“barked”</span>, <span style="color: rgb(0 149 250)">“dog”</span> to <span style="color: rgb(255 89 75)">“barked”</span> and <span style="color: rgb(0 149 250)">“barked”</span> to <span style="color: rgb(255 89 75)">“dog”</span>.</p> <p>Next we compute \(AV\):</p> <figure id="figure-8" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/av_example.png" alt="AV example" style="width: 50%"> <figcaption>Figure 8: AV example </figcaption> </figure> <p>If we look at the weighted feature vector for <em>barked</em>:</p> <figure id="figure-9" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/av_example_worked_out.png" alt="AV example worked out" style="width: 100%"> <figcaption>Figure 9: AV example worked out </figcaption> </figure> <p>We see that because the attention \(a_{barked-dog}\) is the strongest, that the feature vector \(v_{dog}\) is given the most weight when calculating the weighted feature vector for <em>barked</em>.</p> <p>Also note that the weighted feature vector for <span style="color: rgb(0 149 250)">barked</span> now includes information from all other <span style="color: rgb(255 89 75)">keys</span> in the sequence.</p> <h3 id="multi-headed-attention">multi-headed attention</h3> <p>We said that one of the downsides to the RNN with attention was that the decoder hidden state could only ask one “question” of the encoded source sequence. This is because \(qK^T\) results in a \(T\)-dimensional vector \(a\) representing the attention weights. The hidden state only asks a single “question” which gets ranked as a probability distribution against the \(T\) keys in the encoder output.</p> <figure id="figure-10" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/rnn_attention.png" alt="RNN attention" style="width: 100%"> <figcaption>Figure 10: RNN attention </figcaption> </figure> <p>The same is true of the attention mechanism in steps 1 through 3 above. Each token or (“query”) only asks a single question of every other token (“key”). What if we want each token to be able to ask multiple questions of the other tokens and rank alignment for each question separately. These “questions” are learned by the model, but presumably different questions would be “is this an adjective?” or “is this a proper noun?” We can see examples of what kinds of different questions the model has learned to ask in <a href="#attention-examples">attention examples</a>.</p> <p>How can we improve the attention mechanism so that each token asks multiple different questions instead of just a single question? Recall that we map each token to a \(T \times D\) tensor \(Q\), and separately map to a different \(T \times D\) tensor \(K\). To calculate attention, we take the product \(QK^T\) to produce a \(T \times T\) tensor of attention weights, normalized so that each query’s attention maps to a probability distribution for each key. Each query only asks a single question of each key.</p> <p>To improve this so that each query can ask multiple questions of each key, we need to compute multiple \(T \times T\) attention tensors for each question. The number of questions is called \(H\) and is a hyperparameter. Each \(h\) is called a “head” and this strategy is called “multi-head attention.”</p> <p>To construct multiple heads, we split \(Q\), \(K\), and \(V\), all of which are \(\mathbb{R}^{T \times D}\) respectively into \(\mathbb{R}^{T \times H \times D/H}\) tensors, which is equivalent in original size to a \(\mathbb{R}^{T \times D}\) tensor.</p> <figure id="figure-11" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/multi_headed_attention.png" alt="multi-headed attention" style="width: 70%"> <figcaption>Figure 11: Multi-headed attention Q,K,V </figcaption> </figure> <p>Now we take each \(H\) \(Q\) and \(K\) tensors and compute \(A_h\) for each head. This results in \(H\) \(T \times T\) attention weight tensors, instead of the 1 we had previously. This increases the memory requirements by a factor of \(H\). This allows the model to learn \(H\) different measures of similarity between each token.</p> <figure id="figure-12" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/multi_headed_attention_weights.png" alt="multi-headed attention weights" style="width: 70%"> <figcaption>Figure 12: Multi-headed attention weights </figcaption> </figure> <p>Finally, we weight each of the \(H\) value tensors \(V\) using the \(H\) attention weight tensors. This results in \(H\) \(T \times D/H\) tensors which get stacked together as a single \(T \times D\) tensor, the same dimensionality as in the single-headed scenario.</p> <figure id="figure-13" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/multi_headed_feature_vectors.png" alt="multi-headed feature vectors" style="width: 100%"> <figcaption>Figure 13: Multi-headed weighted feature vectors </figcaption> </figure> <p>The stacked multi-headed attention output is on the right.</p> <p>For the cost of \(H\) attention weight tensors instead of 1, the model is able to learn \(H\) different queries, keys, values, and similarity measures.</p> <h4 id="code-example">code example</h4> <p>step 1.</p> <p>In code, using pytorch, if we have a <code class="language-plaintext highlighter-rouge">(T, D)</code> tensor for <code class="language-plaintext highlighter-rouge">Q</code>, <code class="language-plaintext highlighter-rouge">K</code>, and <code class="language-plaintext highlighter-rouge">V</code> respectively, we can split up into \(H\) different heads using a tensor view</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D</span><span class="o">//</span><span class="n">H</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D</span><span class="o">//</span><span class="n">H</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D</span><span class="o">//</span><span class="n">H</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>This will produce a tensor of size <code class="language-plaintext highlighter-rouge">(T, H, D//H)</code>. We swap <code class="language-plaintext highlighter-rouge">T</code> and <code class="language-plaintext highlighter-rouge">H</code> so that <code class="language-plaintext highlighter-rouge">H</code> becomes the batch dimension, which will be needed in the next step, resulting in a <code class="language-plaintext highlighter-rouge">(H, T, D//H)</code> tensor.</p> <p>step 2.</p> <p>To calculate \(H\) \(T \times T\) attention tensors,</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">K</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
</code></pre></div></div> <p>Note that in pytorch if we use <code class="language-plaintext highlighter-rouge">@</code> which is the overloaded symbol for <code class="language-plaintext highlighter-rouge">matmul</code>, that if we have a <code class="language-plaintext highlighter-rouge">(..., A, B)</code> dimensional tensor multiplied with <code class="language-plaintext highlighter-rouge">(..., B, A)</code> tensor, then the <code class="language-plaintext highlighter-rouge">...</code> dimensions are used as a batch dimension and the remaining dimensions are multiplied as in the typical 2d case. Here <code class="language-plaintext highlighter-rouge">H</code> is the batch dimension, but this would usually be part of a mini-batch in which case <code class="language-plaintext highlighter-rouge">B, H</code> would be the batch dimension. This gives us <code class="language-plaintext highlighter-rouge">H</code> <code class="language-plaintext highlighter-rouge">(T, T)</code> tensors, which are calculated in parallel.</p> <p>We map the attention weights for each query to a probability distribution using <code class="language-plaintext highlighter-rouge">softmax</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>step 3.</p> <p>Finally, we obtain the weighted feature vectors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">V</span>
</code></pre></div></div> <p>This uses the same batch matmul procedure as in the attention calculation. We are multiplying <code class="language-plaintext highlighter-rouge">(H, T, T)</code> tensor with <code class="language-plaintext highlighter-rouge">(H, T, D/H)</code> so this produces a <code class="language-plaintext highlighter-rouge">(H, T, D/H)</code> tensor.</p> <p>We stack the <code class="language-plaintext highlighter-rouge">H</code> weighted feature tensors together</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">contiguous</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
</code></pre></div></div> <p>which produces the final <code class="language-plaintext highlighter-rouge">(T, D)</code> tensor.</p> <div class="note-box"> <strong>Note:</strong><br> <p>There is an even more optimized code path to calculate all this using <code class="language-plaintext highlighter-rouge">torch.nn.functional.scaled_dot_product_attention</code> which uses optimized cuda kernels shown to be much faster and with less memory footprint than the above naive implementation</p> </div> <h3 id="decoder">decoder</h3> <p>So far we’ve been looking at the left-side of the transformer diagram, at the encoder. The decoder plays a similar role to the decoder in the encoder-decoder RNN—it decodes the learned representation from the encoder into a sequence. Like the RNN decoder, the transformer decoder will output a single token at each timestep \({1 \ldots T}\) iteratively. It is similar in many ways to the encoder. It will use self-attention to attend to the tokens it has output and will output a \(d\)-dimensional vector at each timestep. Unlike the encoder:</p> <ul> <li>We don’t want the decoder to see into the future during training. This is accomplished using <em>masked attention</em> </li> <li>We want the decoder to also attend to the encoder outputs. This is known as <em>cross-attention</em> </li> </ul> <h4 id="masked-attention">masked attention</h4> <p>Let’s take a look at the <span style=" border: 4px solid black; background-color: rgb(250, 219, 179); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Masked<br> Multi-Head<br> Attention </span> box in the <a href="#transformer-architecture">Transformer architecture</a> diagram. In the RNN decoder, we don’t peek into the future by its nature, since we process each token sequentially. This is the so-called “autoregressive” modeling. We need to be careful that the transformer decoder doesn’t see into the future during training, since the self-attention mechanism is able to look at all tokens, including future tokens. In other words, in the target sequence <code class="language-plaintext highlighter-rouge">[I, have, a, cat]</code>, we do not want <code class="language-plaintext highlighter-rouge">I</code> to “know about” <code class="language-plaintext highlighter-rouge">have a cat</code>. This is because we do not know what the correct output is when this is used on novel input sequences, and letting the model see into the future would make it incapable of generating novel correct sequences. How are we going to accomplish this?</p> <p>The self-attention mechanism allows every query \(q_{1 \ldots T}\) to attend to every key \(k_{1 \ldots T}\). But we only want query \(q_t\) to attend to keys \(k_{1 \ldots t-1}\).</p> <p>To accomplish this, we can zero out attention weights for keys \(k_{t \ldots T}\). That way when we compute the softmax over each key, these keys won’t contribute.</p> <p>The trick, in code, is to set the attention output \(AV\) to \(-\inf\) using an upper triangular matrix for all \(k_{t_k&gt;=t_q}\) where \(t_k\) is the key timestep and \(t_q\) is the query timestep. The attention masking is commonly referred to as <em>causal mask</em>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">t_q</span><span class="p">,</span> <span class="n">t_k</span><span class="p">))</span>
<span class="n">causal_mask</span> <span class="o">=</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">.</span><span class="nf">bool</span><span class="p">(),</span> <span class="o">-</span><span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">))</span>
<span class="n">att</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">k</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">att</span> <span class="o">+=</span> <span class="n">causal_mask</span>
<span class="n">att</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">att</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>Example:</p> <figure id="figure-14" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/masked_self_attention.png" alt="masked self-attention" style="width: 60%"> <figcaption>Figure 14: Masked self-attention </figcaption> </figure> <p>In step <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 1 </span> we initialize \(A\) to hypothetical values where queries are in <span style="color: rgb(10, 146, 240);">blue</span> and keys in <span style="color: rgb(254, 89, 75);">red</span>. In step <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 2 </span> we initialize our upper triangular matrix \(\text{mask}\) to have the value \(-\inf\) in the nonzero slots, and is the same size as \(A\). In step <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 3 </span> we add \(\text{mask}\) to \(A\). In step <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 4 </span> we take the \(\text{softmax}\) across columns for each row. Note the effect of the mask. For the first token <em>I</em> all other timesteps are zeroed out. For the second token, <em>have</em> it is able to attend to itself and the previous token <em>I</em>. For the third token <em>a</em>, it is able to attend to itself and <em>I</em>, <em>have</em>, but not to the next token <em>cat</em>, etc.</p> <h4 id="cross-attention">cross attention</h4> <p>Using masked self-attention, the decoder can pay attention to the tokens in the target sequence. However, we also need the decoder to pay attention to the encoder’s representation of the source sequence. To do this, we’ll use the concept of <em>cross-attention</em> which is the <span style=" border: 4px solid black; background-color: rgb(250, 219, 179); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Multi-Head<br> Attention </span> box in the decoder (right half) of the <a href="#transformer-architecture">Transformer architecture</a> diagram.</p> <p>Using self-attention, a sequence can pay attention to itself. However, we can also use attention so that a sequence \(A\) can pay attention to another sequence \(B\). We’ll use that so that the decoder can pay attention to the encoder’s representation.</p> <p>Let the source sequence be <code class="language-plaintext highlighter-rouge">[J'ai, un, chat]</code> and the target sequence be <code class="language-plaintext highlighter-rouge">[I, have, a, cat]</code>.</p> <figure id="figure-15" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/cross_attention.png" alt="cross attention" style="width: 90%"> <figcaption>Figure 15: Cross attention example </figcaption> </figure> <p>In this example, <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 1 </span> let \(AV_{target}\) be \(AV\) for the target sequence and <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 2 </span> let \(Y_{source}\) be the output from the encoder for the source sequence. Again, let <span style=" display:inline-block; width:20px; height:20px; background:rgb(0 149 250) ; vertical-align:middle; "></span> denote a value for a <span style="color: rgb(0 149 250)">query</span> and <span style=" display:inline-block; width:20px; height:20px; background:rgb(255 89 75) ; vertical-align:middle; "></span> denote a value for a <span style="color: rgb(255 89 75)">key</span>. Then in <span style=" display: inline-flex; justify-content: center; align-items: center; width: 1.4em; height: 1.4em; border-radius: 50%; background: black; color: white; font: 0.9em/1 Helvetica, sans-serif; "> 3 </span> we compute attention weights between the queries from the target to the keys in the source. This is a hypothetical example, but \(att_{target,source}\) is saying that “I” attends most strongly to “Jai,” “have” to “un,” etc.</p> <p>This shows how the target sequence can attend to the source sequence, crucial for translation in which we need to know how the target translation aligns with the source sequence.</p> <h2 id="other-components-of-the-transformer">Other components of the transformer</h2> <p>There are a few other components to the transformer. We’ll cover <span style=" border: 4px solid black; background-color: rgb(183, 228, 244); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Feed Forward </span>, positional encoding, <span style=" border: 4px solid black; background-color: rgb(239, 243, 188); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Add &amp; Norm </span>, <span style=" border: 4px solid black; background-color: rgb(214, 218, 236); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Linear </span>, and <span style=" border: 4px solid black; background-color: rgb(195, 228, 202); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Softmax </span> next.</p> <h3 id="feed-forward-network">Feed forward network</h3> <p>Recall that the output of the attention mechanism is \(AV \in \mathbb{R}^{T \times D}\). At this point there is no nonlinearity, and so the model is limited in its representational capacity. We add a position-wise feed forward network with a single hidden layer, where \(x\) the attention output.</p> \[\begin{aligned} \mathrm{FFN}(x) &amp;= W_2\bigl(\sigma\bigl(W_1\,x + b_1\bigr)\bigr) + b_2, \\[6pt] W_1 &amp;\in \mathbb{R}^{H\times D}, \quad W_2 \in \mathbb{R}^{D\times H}, \\[6pt] \sigma(\cdot) &amp;: \quad (\text{activation function, e.g., ReLU or GELU}) \end{aligned}\] <p>This whole sequence \(AV\) followed by \(\mathrm{FFN}\) is repeated \(N_x\) times, where the sequence is considered a “block” and the output of \({block}_l\) becomes the input of \({block}_{l+1}\), in other words the model starts attending to its own feature representations.</p> <h3 id="positional-encoding">positional encoding</h3> <p>The attention mechanism treats each token independently. In other words, <em>it does not care or no about order</em>. This is known as a “bag of words” model because it’s as if the words were put in a bag and mixed up. This is a problem because the sentences “this is a bad bagel” and “is this a bad bagel” would be identical. Let’s take a look at an example.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/2025-04-13-sequence_to_sequence_models_2/positional_encoding.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>As we can see, when we did not use positional encoding the attention output <code class="language-plaintext highlighter-rouge">y_perm</code> is permuted in the same way as the input <code class="language-plaintext highlighter-rouge">x_perm</code>. Therefore, both orderings would be treated the same by the model.</p> <p>However, when we add a positional encoding by adding <code class="language-plaintext highlighter-rouge">positional_embedding</code> to <code class="language-plaintext highlighter-rouge">x</code>, then order matters since when we shuffle <code class="language-plaintext highlighter-rouge">x</code>, now the <code class="language-plaintext highlighter-rouge">positional_embedding</code> indicates the order and <code class="language-plaintext highlighter-rouge">y_with_pos</code> and <code class="language-plaintext highlighter-rouge">y_shuffle_with_pos</code> are different.</p> <p>The <code class="language-plaintext highlighter-rouge">positional_embedding</code> is just a learned embedding lookup for each token position. One concern is that the positional embedding must be learned and might not be well-trained for token positions that fall outside the normal range (e.g., unusually long sequences). <a class="citation" href="#10.5555/3295222.3295349">(Vaswani et al., 2017)</a> use a non-learned sinusoidal function to add positional information instead, but they didn’t notice any performance difference between the sinusoidal function vs. the positional embedding. Future models such as GPT-2 use positional embedding.</p> <p>Next, we’ll cover <span style=" border: 4px solid black; background-color: rgb(239, 243, 188); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Add &amp; Norm </span>, which is two steps: layer normalization and residual connections.</p> <h3 id="layer-norm">layer norm</h3> <p>For certain types of models, including CNNs, batch normalization <a class="citation" href="#Ioffe_Szegedy_2015">(Ioffe &amp; Szegedy, 2015)</a> is typically used to stabilize the network, and this has been shown to decrease the number of iterations it takes a deep model to converge. Discussion about normalization techniques is outside the scope of this article, but I wanted to motivate why a different normalization technique is used for transformers than was previously used.</p> <p>If we were to apply batch normalization in one dimension for sequential text data where the shape is <code class="language-plaintext highlighter-rouge">(B, T, D)</code>, the normalization is applied for each feature over each <code class="language-plaintext highlighter-rouge">(B, T)</code>, yielding a <code class="language-plaintext highlighter-rouge">D</code>-dimension \(\mu\) and \(\sigma^2\) vector, containing the statistics for each feature over batch and time, as well as <code class="language-plaintext highlighter-rouge">D</code>-dimensional \(\gamma\) and \(\beta\) to scale and shift the distribution. Normalization is applied over the batch and time dimension:</p> \[\mu_d \;=\; \frac{1}{B\,T} \sum_{b=1}^{B}\sum_{t=1}^{T} x_{b,t,d} \qquad \sigma_d^2 \;=\; \frac{1}{B\,T} \sum_{b=1}^{B}\sum_{t=1}^{T} \bigl(x_{b,t,d}-\mu_d\bigr)^{2} \qquad \hat x_{b,t,d} \;=\; \frac{x_{b,t,d}-\mu_d}{\sqrt{\sigma_d^2}} \qquad y_{b,t,d} \;=\; \gamma_d\,\hat x_{b,t,d} + \beta_d\] <figure id="figure-16" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/batch_norm_1d.png" alt="embedding" style="width: 90%"> <figcaption>Figure 16: BatchNorm1d </figcaption> </figure> <p>This is a problem for sequential text data for several reasons.</p> <ol> <li>Each token’s features represent different things, and it’s not natural to normalize over the features of different tokens.</li> <li>This cannot be used when sequences are of different lengths, since in that case the sequences are padded to the longest sequence in the mini-batch, and we don’t want to include padding features when computing statistics. We would have to mask this out somehow.</li> <li>At inference time when we decode the sequence sequentially, we’d be using running statistics that were calculated based on an entire sequence.</li> </ol> <p>For sequential text data, instead, we’d like to normalize each timestep individually, without the features from other timesteps being included. This is what <a class="citation" href="#Ba_Kiros_Hinton_2016">(Ba et al., 2016)</a> does. Instead of normalizing along the batch and time dimension as in BatchNorm1d, layer norm normalizes along each feature axis \(D\) for each timestep, resulting in a <code class="language-plaintext highlighter-rouge">D</code>-dimension \(\mu\) and \(\sigma^2\) vector, containing the statistics for each feature as well as <code class="language-plaintext highlighter-rouge">D</code>-dimensional \(\gamma\) and \(\beta\) to scale and shift the distribution, <em>for each timestep</em>. Every timestep in the batch is normalized individually:</p> \[\mu_{b,t} \;=\; \frac{1}{D} \sum_{d=1}^{D} x_{b, t,d} \qquad \sigma_{b,t}^2 \;=\; \frac{1}{D} \sum_{d=1}^{D} \bigl(x_{b, t,d}-\mu_{b,t}\bigr)^{2} \qquad \hat x_{b,t,d} \;=\; \frac{x_{b,t,d}-\mu_{b,t}}{\sqrt{\sigma_{b,t}^2 + \varepsilon}} \qquad y_{b,t,d} \;=\; \gamma_d\,\hat x_{b,t,d} + \beta_d\] <figure id="figure-17" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/layer_norm.png" alt="embedding" style="width: 90%"> <figcaption>Figure 17: Layer Norm </figcaption> </figure> <p>Now we can normalize over each token individually, and so:</p> <ol> <li>Layer norm normalizes over each timestep individually, instead of including information from other timesteps.</li> <li>We can handle sequences of arbitrary lengths since we normalize each timestep individually.</li> <li>We don’t need to track running statistics, since each timestep is normalized on the fly.</li> </ol> <p>We can also see that experimentally, layer norm gives faster convergence for sequential text data, where in this case layer norm is used in a multimodal model to model images and captions:</p> <figure id="figure-18" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/layer_norm_faster_convergence.png" alt="embedding" style="width: 50%"> <figcaption>Figure 18: Layer Norm faster convergence. Figure from <a class="citation" href="#Ba_Kiros_Hinton_2016">(Ba et al., 2016)</a>. </figcaption> </figure> <h3 id="residual-connections">residual connections</h3> <p>Residual connections (aka skip connections) allow information to flow through the network. <a class="citation" href="#He_resnet">(He et al., 2016)</a> observed that increasing the size of the network leads to worse training performance, which did not make sense, since deeper networks should be able to fit strictly more complex functions than shallower networks. He et al. observed this was because the signal was getting “lost.” Let’s look at a basic example.</p> <p>Consider \(y_1 = f_1(x)\) where \(f_1\), which is a blackbox function, and \(y_2 = f_2(y_1)\) which takes as input \(y_1\).</p> <p>We want to compute \(\frac{\partial y_2}{\partial x}\), i.e., how does an inner input affect the output, which is \(\frac{\partial y_2}{\partial x} = \frac{\partial f_2}{\partial y_1}\frac{\partial y_1}{\partial x}\) due to the chain rule.</p> <p>If the upstream gradient \(\frac{\partial f_2}{\partial y_1}\) is \(&lt;&lt; 1\) or 0, then \(\frac{\partial y_2}{\partial x}\) will be diluted or 0.</p> <p>Residual connections are beautifully simple but powerful. If instead of computing \(y_2 = f_2(y_1)\), we compute \(y_2 = y_1 + f_2(y_1)\), then</p> \[\begin{align*} \frac{\partial y_2}{\partial x} &amp;= (1 + \frac{\partial f_2}{\partial y_1})\frac{\partial y_1}{\partial x} \\ &amp;= \frac{\partial y_1}{\partial x} + \frac{\partial f_2}{\partial y_1}\frac{\partial y_1}{\partial x} \end{align*}\] <p>Notice that just by adding the input \(y_1\), we have made sure that the inner gradient is propagated without being diluted/zeroed out by the upstream gradient.</p> <p>Why are they called “residual” connections? Because now if the downstream layer \(f_2\) contributes meaningfully to the signal, then the term \(\frac{\partial f_2}{\partial y_1}\frac{\partial y_1}{\partial x}\) gets added to the gradient \(\frac{\partial y_1}{\partial x}\); otherwise we still propagate the inner term \(\frac{\partial y_1}{\partial x}\) which tells us how the input affected an earlier layer in the network.</p> <p>In the transformer we use residual connections in the following way:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>The input <code class="language-plaintext highlighter-rouge">x</code> is added to the output in order to propagate the gradient of the input. Notice that the gradient must flow through the layer norm. <a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a> studied if it is better to allow the gradient of the input to flow without it being modified by the layer norm. The original transformer “between residual block” layer norm is referred to as “post-layer normalization,” while the approach by Xiong et al. is “pre-layer normalization.”</p> <p>In code the pre-layer normalization looks like</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer_norm</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p><a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a> found that placing the layer norm within the residual blocks allows for a faster convergence than if the layer norm is placed between the residual blocks. This is because when the layer norm is placed between the residual blocks, then the training requires a learning rate warmup to achieve good results. Without the warmup, training converges more quickly, and <a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a> shows that learning rate warmup is not needed if we place the layer norm inside the residual blocks. The former approach is called <em>post-layer normalization</em> transformer while the latter is <em>pre-layer normalization</em> transformer.</p> <p>They have a nice picture showing this:</p> <figure id="figure-19" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/pre_vs_post_ln_residual_connections.png" alt="pre vs post ln" style="width: 50%"> <figcaption>Figure 19: Pre vs post layer norm. Figure from <a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a>. </figcaption> </figure> <p>Notice that compared to post-layer norm, here the gradient of <code class="language-plaintext highlighter-rouge">x</code> flows through without the gradient being modified by the layer norm. This is what yields faster convergence without learning rate warmup.</p> <p><a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a> showed this experimentally, where we can see clearly that the pre-layer norm without warmup converges faster than the post-layer norm with warmup:</p> <figure id="figure-20" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/pre_vs_post_ln.png" alt="pre vs post ln" style="width: 90%"> <figcaption>Figure 20: Pre vs post layer norm. Figure from <a class="citation" href="#10.5555/3524938.3525913">(Xiong et al., 2020)</a>. </figcaption> </figure> <p>Because of this, transformer architectures such as GPT adopted pre-layer norm (although with warmup, go figure)</p> <h3 id="putting-it-all-together">Putting it all together</h3> <p>We want the decoder to predict <em>the next token</em> \(y_{t+1}\) at each timestep \(t\). To do this, we shift each token 1 unit to the right and fill the token at \(t=0\) with a start of sequence token. The target sequence <code class="language-plaintext highlighter-rouge">[I, have, a, cat]</code> becomes <code class="language-plaintext highlighter-rouge">[&lt;sos&gt;, I, have, a, cat]</code> so that \(y_0\) is <code class="language-plaintext highlighter-rouge">&lt;sos&gt;</code>, \(y_1\) is <code class="language-plaintext highlighter-rouge">I</code>, etc.</p> <p>We feed the source sequence through the encoder, which produces an encoded representation, and the shifted target sequence through the decoder, which also makes use of the encoded representation of the source sequence using cross-attention. The output of this is a \(T \times D\) tensor. But we are not quite done. We need to learn to predict the target sequence from the source sequence. The \(T \times D\) tensor output by the decoder needs to be mapped to \(T \times C\) where \(C\) is the number of tokens in the vocabulary, and for that we’ll use a \(D \times C\) fully connected layer at the end, which is indicated by the <span style=" border: 4px solid black; background-color: rgb(214, 218, 236); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Linear </span> box in the <a href="#transformer-architecture">Transformer architecture</a> diagram. Recall that the token embedding lookup maps \(C\) tokens to \(D\)-dimensional vectors. Because the final fully connected layer can be thought of as the opposite, mapping \(D\) to \(C\) instead of \(C\) to \(D\), we can reuse the weights from the embedding lookup. This saves a lot of extra parameters since \(D\) and \(C\) can be quite large. The logits from the fully connected layer are run through the <span style=" border: 4px solid black; background-color: rgb(195, 228, 202); color: black; padding: 0.1em 0.5em; border-radius: 8px; display: inline-block; text-align: center; font-size: 14px; "> Softmax </span> function to produce a probability distribution over tokens. We then backpropagate using cross-entropy over the \(C\) categories as the loss function.</p> <p>During inference, we will iteratively decode a single token at a time since we do not have a target sequence. After every iteration, we’ll add the decoded token to the decoded sequence to generate the translated sequence, and we’ll stop once we’ve predicted the end-of-sequence token or hit a maximum number of iterations.</p> <h1 id="experiments-and-results">Experiments and results</h1> <p>I implemented a transformer using the approach described in Vaswani et al., to reproduce results, and trained on the WMT ‘14 train set and tested on the WMT ‘14 test set. I was interested in reproducing results in the paper but also wanted to see how the runtime and performance of the model compares to the RNN. Why is the transformer considered a breakthrough? Transformers allow better parallelization than RNNs since we do not need to iterate through each token to process a sequence as in an RNN, but can instead process the entire sequence in parallel using tensor multiplication. So this ought to be more efficient, but how much more efficient? This efficiency, I think, is what enables transformers to perform much better than RNNs at scale since we can train on much larger datasets and models more quickly, and I think this is what allowed researchers to test the hypothesis of whether LLMs would “wake up” when the model size + data size increased. Without transformers, I don’t think we would have reached this paradigm shift.</p> <p>My primary interest was in reproducing the results reported in Vaswani et al.</p> <p>My secondary interest was in seeing how the transformer performance in terms of accuracy as well as runtime and scalability compares to the RNN performance.</p> <p>Lastly, the transformer architecture uses an encoder-decoder architecture; however, GPT and other LLMs use a decoder-only architecture. They use a decoder-only architecture since they train in a different self-supervised regime where they take a large amount of text and just try to predict the next token. This is different from the translation problem in which we want to take a sequence and predict an entire sequence correctly. But I was motivated to try using a decoder-only model for the problem of translation since a decoder-only model is simpler than an encoder-decoder. It also seems easy to frame the translation as predicting the next token rather than encoding the source sequence and then producing the translation.</p> <h2 id="dataset">Dataset</h2> <p>See <a href="https://aamster.github.io/blog/2024/sequence_to_sequence_models_1/#dataset">the previous post</a></p> <h2 id="model">Model</h2> <p>The model hyperparameters are the same as Transformer base in Vaswani et al.</p> \[\begin{array}{|c|c|c|c|c|c|} \hline d_{\text{model}} &amp; h &amp; P_{drop} &amp; d_{ff} &amp; N &amp; \epsilon_{ls} \\ \hline 512 &amp; 8 &amp; 0.1 &amp; 2048 &amp; 6 &amp; 0.1 \\ \hline \end{array}\] <p>where</p> <ul> <li>\(d_{model}\) is the hidden dim in every layer of the model</li> <li>\(h\) is the number of attention heads, and so the hidden dim in every head is \(512/8=64\)</li> <li>\(P_{drop}\) is the dropout</li> <li>\(d_{ff}\) is the feedforward hidden dim</li> <li>\(N\) is the number of blocks</li> <li>\(\epsilon_{ls}\) is the label smoothing strength</li> </ul> <p>A difference between my model and the transformer base model is that I used a separate vocabulary and embedding lookup for both the encoder and decoder, since that was the approach I followed for the RNN model. I used a vocab size of \(30,000\) for both the source and target languages, compared to a vocab size of \(32,000\) total vocabulary size used by Vaswani et al.</p> <p>The model had \(74M\) parameters compared to \(65M\) in transformer base, with the difference stemming from the \(60,000\) embedding vectors compared to \(32,000\).</p> <h2 id="training">Training</h2> <ul> <li>Byte-pair encoding was used for tokenization. See <a href="https://aamster.github.io/blog/2024/sequence_to_sequence_models_1/#tokenization">the previous post</a> for details.</li> <li>Data was split randomly into train/val using 99.9% of the data for train (4000 val examples)</li> <li>I used 3 nvidia V100 GPUs for 24 hours with a batch size of 128 (228,000 iterations).</li> <li>Cross-entropy loss was used, and the model’s weights were updated if the cross-entropy loss decreased.</li> <li>Training was done using float16 mixed precision using automatic mixed precision provided by pytorch (<code class="language-plaintext highlighter-rouge">torch.amp</code>)</li> <li>\(\text{AdamW}\) optimizer using cosine annealing with warmup learning rate scheduler was used. Warmup number of steps is \(2000\), min learning rate is \(6e^-5\), and the max learning rate is \(6e^-4\)</li> </ul> <h2 id="results-and-discussion">Results and discussion</h2> <table id="tbl-bleu"> <caption> The reproduced model achieves equivalent test-set performance as Transformer base. FLOPs calculation is using $$C = 6ND$$ per Kaplan et al. 2020. </caption> <thead> <tr> <th>Model</th> <th>Non-embedding Parameters</th> <th>FLOPs</th> <th>BLEU</th> </tr> </thead> <tbody> <tr> <td>Transformer Base</td> <td>65M</td> <td>$$1.95 \times 10^{18}$$</td> <td>38.1</td> </tr> <tr> <td>This model</td> <td>60M</td> <td>$$1.57 \times 10^{18}$$</td> <td>38.6</td> </tr> </tbody> </table> <p>The test set performance shows that we were able to achieve the same performance.</p> <p>From the plot of train/validation BLEU score, we can see that the model did not converge and could have continued training with increased performance.</p> <figure id="figure-21" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/encoder_decoder_perf.png" alt="masked self-attention" style="width: 90%"> <figcaption>Figure 21: Transformer performance </figcaption> </figure> <h3 id="attention-examples">Attention examples</h3> <p>Let’s look at the encoder self-attention, and decoder-cross attention calculated by the model. I’m skipping decoder self-attention as I don’t speak French, and it is most likely similar in nature to the encoder self-attention. I’ve chosen the same running example from the test set “But why such optimism for some and pessimism for others?” → “Les raisons d’un tel optimisme, chez les uns, et pessimisme, chez les autres ?”.</p> <h4 id="encoder-self-attention">Encoder self-attention</h4> <p>I’ve broken down the self-attention by layer which can be toggled by the slider at the bottom, and each subplot is a single head of attention. The query tokens are on the left and key tokens on the right. The line weight indicates the attention weight ranging from \([0, 1]\).</p> <figure id="figure-22" style="text-align: center;"> <div id="encoder_self_attention_weights_plot_container" style="width: 100%"></div> <script>fetch("/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/multi_head_encoder_self_attention_weights_0.json").then(t=>t.json()).then(t=>{Plotly.newPlot("encoder_self_attention_weights_plot_container",t.data,t.layout)})["catch"](t=>{console.error("Error loading Plotly data:",t)});</script> </figure> <p>Here’s what I notice, focusing on a couple layers for brevity:</p> <ul> <li>Layer 1 <ul> <li>In layer 1 the attention seems pretty diffuse where each token in general seems to be attending to every other token</li> </ul> </li> <li>Layer 2 <ul> <li>In layer 2 we start to see more specialized heads. In head 2 token \(j\) is attending to the previous token \(j-1\), and in head 4 token \(j\) attends to the next token \(j+1\).</li> <li>Interestingly, many of the tokens are attending to the end of sequence token <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>.</li> </ul> </li> <li>Layer 6 <ul> <li>Head 7 <em>?</em> attends to <em>why</em> </li> <li>Head 8 token \(j\) attends to itself</li> <li>Head 6 some tokens attend to another instance of itself elsewhere in the sequence, e.g., the first <em>for</em> attends to the 2nd <em>for</em>, or if no other instance is in the sequence, then it attends to itself</li> </ul> </li> </ul> <p>As we can see, the heads are performing different functions in terms of learning the structure of the sequence.</p> <h4 id="decoder-cross-attention">Decoder cross-attention</h4> <p>The following example is generated by collecting attention weights from the decoder during inference generation of the sequence.</p> <figure id="figure-23" style="text-align: center;"> <div id="decoder_cross_attention_weights_plot_container" style="width: 100%"></div> <script>fetch("/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/multi_head_decoder_cross_attention_weights_0.json").then(t=>t.json()).then(t=>{Plotly.newPlot("decoder_cross_attention_weights_plot_container",t.data,t.layout)})["catch"](t=>{console.error("Error loading Plotly data:",t)});</script> </figure> <p>A few observations:</p> <ul> <li>Throughout all layers a significant amount of attention is given to the end of sequence token <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>, particularly in early layers 1-3</li> <li>Some heads, e.g., head 7 in layer 3 align output tokens with the corresponding input token (Mais -&gt; But, porquois -&gt; why, etc). This makes sense and what I would expect the model to do for translation, since the output needs to be aligned with the input.</li> <li>Other heads, e.g., head 1 in layer 5 align output tokens with the corresponding input token that follows it (in the phrase “Mais porquois” (But why), “Mais” (but) attends to why,” etc.)</li> </ul> <p>Many of these observations on attention behaviors are not unique to this data/model but are found in other research and LLMs, such as in <a class="citation" href="#Clark_Khandelwal_Levy_Manning_2019">(Clark et al., 2019)</a> in which they also find that a significant amount of attention weight is placed on <em>[SEP]</em>, the token used to indicate a sequence boundary (similar to <code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code> that I used). They argue that this is a “no-op” or a sort of fallback behavior. They also find that many heads do simple and similar things such as just attending to the previous or the next token.</p> <p>Contrary to what the paper observes, heads in the same layer don’t always perform the same function, e.g., head 2 and 4 in layer 2 of the encoder self-attention clearly perform previous token and next token alignment respectively.</p> <h3 id="transformer-vs-rnn-architecture">Transformer vs RNN architecture</h3> <p>One of the motivating reasons for studying the transformer was to try to understand the benefits it has over the RNN. We know that researchers and AI companies have adopted the transformer architecture for LLMs, and so we’d like to try to understand why LLMs weren’t built with a different architecture, such as the RNN.</p> <p>Here is a summary table comparing RNN with Transformer efficiency and performance.</p> <table> <thead> <tr> <th>Model</th> <th style="text-align: right">Parameter count<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> </th> <th style="text-align: right">Num. tokens processed<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> </th> <th style="text-align: right">Num. GPU hours<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> </th> <th style="text-align: right">GPU</th> <th style="text-align: right">FLOPs<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup> </th> <th style="text-align: right">FLOP/s</th> <th>BLEU<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> </th> </tr> </thead> <tbody> <tr> <td>RNN</td> <td style="text-align: right">158M</td> <td style="text-align: right">6.1 B</td> <td style="text-align: right">159</td> <td style="text-align: right">nvidia V100</td> <td style="text-align: right">\(5.78 \times 10^{18}\)</td> <td style="text-align: right">\(1 \times 10^{13}\)</td> <td>.30</td> </tr> <tr> <td>Transformer</td> <td style="text-align: right">60M</td> <td style="text-align: right">4.37 B</td> <td style="text-align: right">30</td> <td style="text-align: right">nvidia V100</td> <td style="text-align: right">\(1.57 \times 10^{18}\)</td> <td style="text-align: right">\(1.45 \times 10^{13}\)</td> <td><strong>.38</strong></td> </tr> </tbody> </table> <h4 id="runtime-efficiency">runtime efficiency</h4> <p>First, which model ran faster? The RNN must process each token sequentially, while the transformer can process all tokens in parallel using matrix multiplications, and so we would expect the transformer to make better use of the hardware. Deep learning model’s runtime complexity is typically measured in terms of FLOPs (floating point operations), rather than using big-O notation as is typical for algorithms. Neural networks essentially do many adds and multiplies. A single add and a single multiply would count as 2 FLOPs. A matrix multiply \(AB\) where \(A\) is \([M, N]\) and \(B\) is \([N, P]\) would require \(2*M*N*P\) FLOPs. If \(A\) is \([49, 1]\) and \(B\) is \([1, 10000]\) then there are \(980,000\) FLOPs. Models that require more FLOPs are computationally more expensive.</p> <p>How are FLOPs calculated? In <a class="citation" href="#10.5555/3295222.3295349">(Vaswani et al., 2017)</a> they calculate FLOPs</p> <blockquote> <p>by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU</p> </blockquote> <p>This relies on the floating point capacity of the GPU and assumes 100% or near 100% saturation of the GPU and is GPU-dependent. If the model doesn’t 100% saturate the GPU, then this calculation will be an overestimate.</p> <p>Later work <a class="citation" href="#Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020">(Kaplan et al., 2020)</a> derived that for most neural network models the number of FLOPs can be calculated as \(C=6ND\) where N is the number of parameters in the model and D is the number of inputs (see <a href="https://jax-ml.github.io/scaling-book/transformers/" rel="external nofollow noopener" target="_blank">this blog post</a> for an explanation). Using this method to estimate the number of FLOPs, the transformer performed \(1.57 \times 10^{18}\) of FLOPs while the RNN performed \(5.78 \times 10^{18}\) FLOPs. The transformer trained for 30 GPU hours and performed \(1.45 \times 10^{13}\) FLOP/s while the RNN trained for 159 GPU hours and performed \(1 \times 10^{13}\) FLOP/s, so <strong>the transformer was able to run 1.45x faster than the RNN</strong>.</p> <p>This is also supported by looking at GPU utilization logs. Since the transformer keeps the GPU busier by processing tokens in parallel, it is able to run more operations / second.</p> <figure id="figure-24" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/gpu_util.png" alt="gpu util" style="width: 60%"> <figcaption>Figure 24: RNN vs. Transformer GPU utilization. The transformer keeps the GPU busier and more saturated. </figcaption> </figure> <h4 id="translation-quality">Translation quality</h4> <p>Recall that the RNN achieved <strong>30</strong> WMT’14 test set BLEU, while the transformer achieved <strong>38</strong> BLEU and so the transformer had better translation quality. It also did this with less data and with fewer weights. The RNN was trained on 6.1B tokens and contained 158M params, while the transformer only trained on 4.7B tokens and contained 60M params.</p> <p>So the transformer achieved significantly better translation quality using fewer parameters and 25% less data, and was 1.45x more efficient in terms of theoretical FLOP/s.</p> <h3 id="encoder-decoder-vs-decoder-only">Encoder-decoder vs decoder-only</h3> <p>The transformer architecture used by LLMs is largely the same as in the original transformer but with a few differences. The differences include:</p> <ul> <li>GELU activation function <a class="citation" href="#Hendrycks_Gimpel_2016">(Hendrycks &amp; Gimpel, 2016)</a> instead of ReLU</li> <li>layer norm within the residual block (pre-LN)</li> <li>additional layer norm before the final linear layer</li> <li>Decoder-only (no encoder)</li> </ul> <p>The first three are fairly simple architectural details that yield an improvement. The last difference, decoder-only vs. encoder-decoder, requires some explanation. In the original transformer paper, the transformer includes both an encoder and a decoder. The encoder encodes the source text while the decoder produces the output. This makes sense since there is a finite input (the input text) that must be understood to produce the output. However, LLM pretraining works by taking in a context of text and predicting the next token. This is what the decoder does in our encoder-decoder model. It takes as context the currently predicted sequence and predicts the next token. What if instead of just taking the target text or predicted text as context, it also prepended the source text? Then we can treat the entire source + target/predicted text as a single sequence and learn to predict the next token.</p> <figure id="figure-25" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/decoder.png" alt="decoder" style="width: 20%"> <figcaption>Figure 25: Decoder (right half) of transformer encoder-decoder diagram </figcaption> </figure> <p>I largely followed <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> which had the same idea.</p> <figure id="figure-26" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/lm4mt.png" alt="lm4mt" style="width: 100%"> <figcaption>Figure 26: Decoder-only model. Diagram from <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a>. During training, the source and target texts are concatenated into a long sequence and the next token is predicted. During inference the source text is given as input, and the predicted text is concatenated to it. </figcaption> </figure> <p>As an example, if the following are the source and target texts, respectively, they would get concatenated into a single long input:</p> <p><strong>concatenated input:</strong> <em><code class="language-plaintext highlighter-rouge">&lt;source&gt;</code> Macedonian entrepreneurs and their descendants eventually employed their numerical strength within the food service industry as a catapult into a variety of larger and more sophisticated ventures. <code class="language-plaintext highlighter-rouge">&lt;target&gt;</code> Finalement, des entrepreneurs macédoniens et leurs descendants utiliseront la force du nombre dans l’industrie du service alimentaire comme un tremplin vers différentes entreprises plus perfectionnées et plus importantes.</em></p> <p>You’ll notice that the source and target texts are separated by a special language tag token, which <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> found to be beneficial.</p> <p>One concern is that the goal, remember, is to translate text. We want our measure of success to be how well the output aligns with the expected output. However, if we were to model the concatenated text next token prediction using cross-entropy loss, then we would also be tuning the model to predict the next token <em>in the source text as well as the target text</em>. We don’t care how well the model can predict the next token for the source text, but we do for the target text.</p> <p>For this reason <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> separated the loss into a multi-task loss with two parts. Recall that in the encoder-decoder, the loss is only measured on the target sequence as the cross-entropy loss</p> \[\mathcal{L}_{CE} = -\sum_{1}^{T}\text{log}P\bigl(y_t \mid \mathbf{x}, \mathbf{y}_{&lt;t}\bigr)\] <p>. However, in the decoder-only setting, the source and target are concatenated. <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> break up the loss into 2 parts: \(\mathcal{L}^{AE}\), the autoencoder loss which is defined as</p> \[\mathcal{L}^{AE} = -\sum_{1}^{S}\text{log}P\bigl(x_s \mid \mathbf{x}_{&lt;s}\bigr)\] <p>and the machine translation loss \(\mathcal{L}^{MT}\) which is defined as</p> \[\mathcal{L}^{MT} = -\sum_{1}^{T}\text{log}P\bigl(y_t \mid \mathbf{x}, \mathbf{y}_{&lt;t}\bigr)\] <p>where \(S\) is the number of tokens in the source sequence, \(T\) is the number of tokens in the target sequence, \(\mathbf{x}\) is the source sequence, and \(\mathbf{y}\) is the target sequence.</p> <p>The total loss \(\mathcal{L}\) is therefore</p> \[\mathcal{L} = \mathcal{L}^{AE} + \mathcal{L}^{MT}\] <p>This loss differs from the transformer loss \(\mathcal{L}_{CE}\) in that we’re computing loss on the source sequence as well as the target sequence.</p> <p>Since we’ve separated the loss into two parts, 1 for the source sequence and another for the target sequence, we can control each separately. Since the goal is translation, we might want to focus more on the \(\mathcal{L}^{MT}\) component at later stages in training. This is what <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> did, they added a term \(\lambda_d\) as a decay factor, which decays piecewise linearly using hyperparameters \(\alpha\) and \(\mathcal{B}\). The loss with the decay factor is then</p> \[\mathcal{L}_{decoder} = \lambda_d\mathcal{L}^{AE} + \mathcal{L}^{MT}\] <h4 id="experiments">Experiments</h4> <p>I compared the model using the transformer loss \(\mathcal{L}_{MT}\) with the model using the weighted multi-task loss \(\mathcal{L}_{decoder}\), and compared both of these with the encoder-decoder transformer. For an equal comparison, since the decoder doesn’t include an encoder, we make up for the lost parameters. <a class="citation" href="#Wang_Tu_Tan_Wang_Sun_Liu_2021">(Wang et al., 2021)</a> did this by increasing the depth of the network. I increased \(N\), the number of layers from 6 to 19. Note that, due to a bug, I originally miscalculated the number of params in the encoder-decoder as 74M instead of 60M, and so made the decoder-only model have 74M params instead of 60M. BLEU is reported on the WMT’14 test set.</p> <table id="tbl-decoder-only-vs-encoder-decoder"> <caption> Decoder-only vs encoder-decoder transformer BLEU scores. The decoder-only model using $$\mathcal{L}_{decoder}$$ outperforms the other models. </caption> <thead> <tr> <th>Model</th> <th>Num non-embed params</th> <th>Num layers</th> <th>Num epochs</th> <th>FLOPs</th> <th>BLEU</th> </tr> </thead> <tbody> <tr> <td>Encoder-decoder</td> <td>60M</td> <td>6</td> <td>2.16</td> <td>$$1.57 \times 10^{18}$$</td> <td>38.6</td> </tr> <tr> <td>Decoder-only with $$\mathcal{L}_{MT}$$</td> <td>76.28M</td> <td>19</td> <td>1.41</td> <td>$$2 \times 10^{18}$$</td> <td>37.3</td> </tr> <tr> <td>Decoder-only with $$\mathcal{L}_{decoder}$$</td> <td>76.28M</td> <td>19</td> <td>1.41</td> <td>$$2 \times 10^{18}$$</td> <td><b>38.7</b></td> </tr> </tbody> </table> <p>Decoder FLOP calculation: \(C = 6 \times 76.28M \times 3.1 \text{B tokens} \times 1.41\)</p> <p>The decoder-only using \(\mathcal{L}_{decoder}\) had the best performance, but performed ~400M more FLOPs than the encoder-decoder due to the parameter difference count. This shows that a decoder-only model, with a few minor adjustments can be used to get good performance on translation tasks. We didn’t test it, but also presumably just using \(\mathcal{L}_{CE}\) on the entire source + target sequence would do ok, since ChatGPT can translate very well with enough data:</p> <figure id="figure-27" style="text-align: center;"> <img src="/assets/img/2025-04-13-sequence_to_sequence_translation/chatgpt_translate.png" alt="chatgpt translate" style="width: 100%"> <figcaption>Figure 27: Example chatgpt translation </figcaption> </figure> <p>which when translated back to English via Google Translate yields:</p> <blockquote> <p>The decoder-only model using the \mathcal{L}_{decoder} loss achieved the best performance. Note that we didn’t train it as long as the encoder-decoder, so we can expect better results if we had continued training for longer.</p> </blockquote> <h3 id="attention-weights">Attention weights</h3> <p>Finally, let’s look at some attention weights from the decoder-only model. The source text is given to the model as inference input, and the predicted tokens are iteratively added to the sequence. We collect the attention weights after completing the predicted sequence. Note that language tags <code class="language-plaintext highlighter-rouge">&lt;en&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;fr&gt;</code> are prepended to the english and French sequences respectively. Only showing weights &gt; 0.1 to reduce the number of lines drawn.</p> <figure id="figure-28" style="text-align: center;"> <div id="decoder_only_attn_weights_container" style="width: 100%"></div> <script>fetch("/assets/plotly/2025-04-13-sequence_to_sequence_translation_2/multi_head_decoder_attention_weights_0.json").then(t=>t.json()).then(t=>{Plotly.newPlot("decoder_only_attn_weights_container",t.data,t.layout)})["catch"](t=>{console.error("Error loading Plotly data:",t)});</script> </figure> <p>We can see some interesting patterns. Here are ones that stand out as the most obvious.</p> <p><strong>Layer 1, head 2</strong>: Some tokens just pay attention to itself while others either pay attention to the same token output previously, e.g. “for” and “for” or the target token is aligned with the source token, e.g. “et” and “and”</p> <p><strong>Layer 5, head 5</strong>: Previous token attention</p> <p><strong>Layer 6, head 1</strong>: Tokens that go together are aligned, e.g. “si-mis-m” attend to “pes” or “tel” attend to “un” in “un tel” meaning “such”</p> <p><strong>Layer 9, head 3</strong>: Many tokens attend to <code class="language-plaintext highlighter-rouge">&lt;en&gt;</code> tag, which seems like a “default” or no-op that we’ve seen previously. This trend is repeated throughout many layers/heads, e.g., layer 18 head 5 attends to both <code class="language-plaintext highlighter-rouge">&lt;en&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;fr&gt;</code> respectively.</p> <p>Overall, we see a mixture of the English sequence attending to its tokens, the French sequence attending to its tokens, and the French sequence attending to the English tokens. A significant number of layers/heads don’t contain useful signal and just attend to <code class="language-plaintext highlighter-rouge">&lt;en&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;fr&gt;</code>.</p> <h1 id="conclusion">Conclusion</h1> <p>We’ve looked at the transformer architecture and how it differs from the RNN architecture that preceded it. The transformer has better performance on the translation task but beyond translation, is the SOTA model for multi-modal understanding and generation. It makes better use the hardware for parallelism and so is faster and more scalable than the RNN architecture. We also looked at how the original encoder-decoder transformer architecture compares to decoder-only LLM architecture and showed that the decoder-only model can achieve similar performance as the encoder-decoder on the original translation task as well. This shows that the original transformer could be formulated as a modern decoder-only transformer without loss in performance.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Excluding embedding layers <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2"> <p>The number of tokens processed is calculated as average tokens per iteration × number of iterations. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3"> <p>Only including train time, not inference/validation <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4"> <p>FLOP calculation is \(C=6ND\) per Kaplan et al. 2020 (see <a href="https://jax-ml.github.io/scaling-book/transformers/" rel="external nofollow noopener" target="_blank">this blog post</a>). <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5"> <p>WMT’14 test set BLEU <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="10.5555/3295222.3295349" class="col-sm-10"> <div class="title">Attention is all you need</div> <div class="author"> Ashish Vaswani, Noam Shazeer, Niki Parmar, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, Long Beach, California, USA, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Ioffe_Szegedy_2015" class="col-sm-10"> <div class="title">Batch normalization: accelerating deep network training by reducing internal covariate shift</div> <div class="author"> Sergey Ioffe, and Christian Szegedy </div> <div class="periodical"> <em>In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37</em>, Lille, France, Jun 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Ba_Kiros_Hinton_2016" class="col-sm-10"> <div class="title">Layer Normalization</div> <div class="author"> Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton </div> <div class="periodical"> Jun 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="He_resnet" class="col-sm-10"> <div class="title">Deep Residual Learning for Image Recognition</div> <div class="author"> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jian Sun' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.5555/3524938.3525913" class="col-sm-10"> <div class="title">On layer normalization in the transformer architecture</div> <div class="author"> Ruibin Xiong, Yunchang Yang, Di He, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 37th International Conference on Machine Learning</em>, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="Clark_Khandelwal_Levy_Manning_2019" class="col-sm-10"> <div class="title">What does Bert Look at? an analysis of Bert’s attention</div> <div class="author"> Kevin Clark, Urvashi Khandelwal, Omer Levy, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Christopher D. Manning' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> Jun 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="Kaplan_McCandlish_Henighan_Brown_Chess_Child_Gray_Radford_Wu_Amodei_2020" class="col-sm-10"> <div class="title">Scaling laws for neural language models</div> <div class="author"> Jared Kaplan, Sam McCandlish, Tom Henighan, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> Jan 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="Hendrycks_Gimpel_2016" class="col-sm-10"> <div class="title">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</div> <div class="author"> Dan Hendrycks, and Kevin Gimpel </div> <div class="periodical"> Jun 2016 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="Wang_Tu_Tan_Wang_Sun_Liu_2021" class="col-sm-10"> <div class="title">Language models are good translators</div> <div class="author"> Shuo Wang, Zhaopeng Tu, Zhixing Tan, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Wenxuan Wang, Maosong Sun, Yang Liu' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> Jun 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/sequence_to_sequence_models_1/">Translation using deep neural networks - RNN (part 1)</a> </li> <div id="giscus_thread" style="max-width: 1300px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"aamster/aamster.github.io","data-repo-id":"R_kgDOM65yIA","data-category":"Comments","data-category-id":"DIC_kwDOM65yIM4CjkU0","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Adam Amster. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?ce13fb97c1d6998ff5b6a52ebe23d8f7"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about"}},{id:"post-translation-using-deep-neural-networks-transformer-part-2",title:"Translation using deep neural networks - Transformer (part 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/sequence_to_sequence_models_2/"}},{id:"post-translation-using-deep-neural-networks-rnn-part-1",title:"Translation using deep neural networks - RNN (part 1)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sequence_to_sequence_models_1/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61.%61%6D%73%74%65%72%31@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=a.amster1@gmail.com","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/adam-amster-a1ba3850","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>