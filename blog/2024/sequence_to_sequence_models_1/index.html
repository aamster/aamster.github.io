<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Sequence to sequence translation (part 1) | </title> <meta name="author" content="Adam Amster"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/moon_avatar.png?ff160fd774b875a0b70dabef23945735"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://aamster.github.io/blog/2024/sequence_to_sequence_models_1/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Sequence to sequence translation (part 1)",
            "description": "",
            "published": "October 03, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about">about </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Sequence to sequence translation (part 1)</h1> <p></p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#"></a> </div> </nav> </d-contents> <p>In this article, I’ll introduce language modeling using deep learning and will focus on the problem of translation.</p> <p>I’ll cover how language was modeled using deep learning prior to transformers, which was by using recurrent neural networks (RNNs).</p> <p>I’ll compare <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> which introduces the idea of attention in RNNs, with a paper published around the same time <d-cite key="10.5555/2969033.2969173"></d-cite> which does not use attention. I’ll explain in detail what attention is and why it was introduced, as well as try to reproduce the results from these papers. The results from these papers are actually contradictory; <d-cite key="10.5555/2969033.2969173"></d-cite> reported better performance than <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, however the latter is well known and influential for introducing attention. This is surprising to me, and shows that researchers found the idea of attention to be useful regardless of the performance reported in these papers.</p> <p>In the next article I’ll cover how language is modeled today using transformers.</p> <h1 id="introduction">Introduction</h1> <h2 id="avoiding-nlp">Avoiding NLP</h2> <p>In school, I always skipped over the natural language processing (NLP) topic for several reasons</p> <p>NLP used to be boring to me and language-specific where linguistics domain knowledge was needed. It involved steps such as</p> <ul> <li>tokenization (which is still needed but there wasn’t as good support for it before and it is annoying to implement),</li> <li>parts of speech tagging</li> <li>entities extraction</li> <li>…, etc</li> </ul> <p>The second reason why I avoided this topic is that I didn’t see much practical value in learning NLP. When would I ever need to build a chatbot or translation model?</p> <p>These reasons lead to me cringing everytime I saw “NLP” mentioned</p> <h2 id="language-modeling-today">Language modeling today</h2> <p>Turn to 2024 and language modeling is arguably the hottest topic in AI, so much so in fact that when some people say “AI” they are actually referring to language modeling.</p> <p>The models used for language modeling are also general purpose models that can handle any problem in which the data can be broken up into a finite sequence of tokens such as image understanding/generation, audio, biology (genomics), etc, and so language is just one use case of these models.</p> <h2 id="what-is-a-language-model">What is a language model?</h2> <p>Simply put a language model is a model that has learned to predict the next token given the previous tokens. The token is usually a word/subword/single character.</p> <h2 id="what-is-a-large-language-model">What is a large language model?</h2> <p>A large language model or LLM is a language model that has been trained on a ton of text and has many parameters (billions or trillions). Parameters are akin to the weights in biological neurons which determine whether they fire or not.</p> <h2 id="why-is-language-modeling-now-relevant-and-interesting">Why is language modeling now relevant and interesting?</h2> <p>Language models are part of a trend in AI of generative models. Generative models produce data, whereas historically machine learning models used to only predict a single variable, such as whether a review was good or the price of a house.</p> <p>Other types of generative models would cover the tasks of image generation, video generation, audio generation, and other kinds of generation. But this article will focus on language modeling.</p> <p>It is an understatement to say that generative models are a hot topic right now (mostly language models). It seems overnight companies are racing to build bigger and better models and other companies are racing to incorporate these models into their product offering. Schools are discussing whether to allow the use of LLMs or how to even prevent them from being used.</p> <p>We’ve seen that GPT-3.5+ models are incredibly powerful. They have seemingly limitless knowledge about seemingly all topics and can create and combine things in novel ways. When GPT-3.5 came out, most people discovered that it can write haikus, <a href="https://www.reddit.com/r/ChatGPT/comments/101huf7/prompt_please_write_a_breaking_news_article_about/" rel="external nofollow noopener" target="_blank">breaking news articles about falling leaves</a>, and get it to say other amusing and funny things.</p> <p>But language models are more than just haiku-generating machines. I frequently use GPT to figure out confusing things in code or an API I’m unfamiliar with, or to do tedius software engineering related things. I also find myself bouncing ideas off of GPT to get unstuck.</p> <p>There is also tremendous potential in teaching and education. While reading a book or watching a video, you can ask GPT to explain something to better understand it; this makes learning more interactive and effective.</p> <p>In research, many people are doing and have been doing amazing research. There is not even close to being enough time for anyone to keep up. Large language models can help surface and summarize research and help people navigate the idea landscape.</p> <p>Much of life can be tedious or confusing; large language models can hopefully serve as the “all knowing” guide and enhance our creativity and insight without us needing to sacrifice our autonomy or intelligence.</p> <h2 id="sequence-to-sequence-translation">Sequence to sequence translation</h2> <p>Translation has historically been a challenging task to automate and was used as a springboard for different AI research.</p> <p>I chose translation as the task to focus on for learning language modeling as 2 seminal papers in language modeling, which I’ll get to, also used translation as the task.</p> <p>The task of translation, as you may have guessed, involves translating an arbitrary document (word, sentence, paragraph, whatever) from 1 language into another.</p> <h2 id="why-is-translation-using-neural-networks-difficult">Why is translation using neural networks difficult</h2> <p>Translation using neural network models is challenging for several reasons.</p> <h3 id="language-involves-sequences-of-arbitrary-length">Language involves sequences of arbitrary length</h3> <p>According to <d-cite key="10.5555/2969033.2969173"></d-cite></p> <blockquote> <p>Despite their flexibility and power, DNNs [Deep Neural Nets] can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality. It is a significant limitation, since many important problems are best expressed with sequences whose lengths are not known a-priori.</p> </blockquote> <p>Today, this issue seems like a non-issue. But back in 2014 this was seen as a novel problem to solve.</p> <p>Traditional machine learning operates on fixed length vectors, such as variables representing imporant features of a house (e.g. # of bedrooms, square footage, location, etc), the length of which is known in advance.</p> <p>Language is different in that sentences can be of arbitrary length. How to encode the sentences “I ate a cheese sandwhich” as well as “It is morning.” such that the model receives a fixed length input?</p> <h3 id="long-inputs">Long inputs</h3> <p>The paragraph</p> <blockquote> <p>It was cold outside so I wanted something warm to drink. I was out of tea so I thought to go to the store to buy some. My favorite kind of tea is matcha tea because of its health benefits. It was on sale so I bought some matcha tea. Drinking the tea really warms me up and gives me energy!</p> </blockquote> <p>contains a lot of information. How do we encode all this information so that we can know how to translate it?</p> <p>In fact, early models would do better at shorter sentences than longer ones because of this issue. The model was able to encode the meaning only for very short inputs but tended to not properly capture longer inputs.</p> <d-cite key="cho-etal-2014-properties"></d-cite> <p>analysis of early DNN translation models states:</p> <blockquote> <p>Our analysis shows that the performance of the neural machine translation model degrades quickly as the length of a source sentence increases.</p> </blockquote> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/rnn_enc_decreasing_perf_sent_len.png" alt="bleu vs sentence len"></p> <p>However, <d-cite key="10.5555/2969033.2969173"></d-cite> states</p> <blockquote> <p>We were surprised to discover that the LSTM did well on long sentences</p> </blockquote> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/sutskever_sent_len.png" alt="Sutskever bleu vs sentence len"></p> <p>And then again in the seminal paper <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, which I’ll discuss in more detail later shows the same problem, that the model struggles with longer sentences:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/bhadanau_perf.png" alt="Bhadanau perf \label{Figure 2}"></p> <p>Notice how the performance (BLEU score, I’ll discuss in more detail later in the article) degrades rapidly as the sentence length increases in the RNNenc-50 and RNNenc-30 models.</p> <p>So there is a discrepancy among these papers when discussing the problem of sentence length. Sutskever et al, 2014 say that the model performs well on long inputs, but Cho et al, 2014 and Bahdanau et al, 2014 say that the models are defficient when the input size increases. All these papers use the same dataset, WMT ‘14 for training and evaluation.</p> <p>The problem of sentence length would lead to the idea of attention in Bahdanau et al, 2014, which forms the basis for the transformer architecture.</p> <h3 id="vocabularies-are-large">Vocabularies are large</h3> <p>In language models, we have to turn the text into a series of tokens. Typically, the tokens were words but in modern LLMs the tokens are constructed using the <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="external nofollow noopener" target="_blank">byte pair encoding algorithm</a> in which the tokens are not necessarily words but can be phrases, sub-words or single characters. I’ll go into more detail about the BPE algorithm later in the article.</p> <p>Once the data is tokenized, then we have to map each token to a vector of numbers which is achieved using an embedding matrix, and at prediction time we have to predict a probability for each of the tokens in our vocabulary. So a large vocabulary increases memory requirements by the model due to the increase in size of the embedding matrix for every token in the vocabulary, as well as the increase in number of neurons for predicting the output token, which is usually a linear transformation.</p> <p>According to the website <a href="https://www.opensourceshakespeare.org/statistics/" rel="external nofollow noopener" target="_blank">open source Shakespeare</a> there are 28,829 unique words in all of Shakespeare’s works, and apparently 40% of those words are used only once. That is a lot of words, but relatively modern LLMs (e.g. GPT2) use a token size of ~50,000 <d-cite key="radford2019language"></d-cite>, since these models need to understand more than just the vocabulary of Shakespeare.</p> <p>Large vocabulary sizes also mean that we need a very large dataset in order to capture enough examples for every token in the vocabulary in order to train each token sufficiently. For example rarer tokens might be undertrained.</p> <h3 id="translation-is-difficult">Translation is difficult</h3> <p>The Hebrew sentence יש לי גרביים means “I have socks”.</p> <p>The literal translation is “There is to me socks”. This means that a language model cannot just learn a mapping between words. If the model needed to translate the english sentence “I have socks” to Hebrew, it cannot just look up the Hebrew word for “I”, which would be “אני”. It needs to know that the 2 words “I” followed by “have” get translated to the Hebrew “יש לי”. On the other hand if the sentence were “I am a man”, then “I” should get translated to “אני”.</p> <p>Another example would be translating the phrase “black cat” into spanish in which case it is “gato negro”. In other words when the model sees “black” it should output “gato” or “cat” and when it sees “cat” it should output “negro” or “black”.</p> <p>This is just scratching the surface as to why translation is difficult.</p> <h1 id="sequence-to-sequence-models">Sequence to sequence models</h1> <p>Translation involves taking in a sequence of text and producing a sequence of text. For example:</p> <p>Input:</p> <p>יש לי גרביים</p> <p>Output:</p> <p>I have socks.</p> <p>The way that we (humans) process language is by starting from the beginning and reading until the end.</p> <p>I—have—socks—.</p> <p>We mentally break up the sentence into chunks and process it sequentially in parts. We also need to understand the meaning of the sentence as a whole.</p> <p>When translating this sentence into a different language, after we mentally process the input, we then start producing the output one chunk at a time.</p> <p>יש—לי—גרביים</p> <p>So this involves 3 main steps:</p> <ol> <li>tokenize the input into a sequence of tokens</li> <li>process and condense/understand the input so that we can translate it</li> <li>use the condensed form from (2) to produce a translation token by token</li> </ol> <p>We would like a machine learning model that can do steps (2) and (3) above.</p> <p>The encoder-decoder architecture as proposed by<d-cite key="10.5555/2969033.2969173"></d-cite> does just this.</p> <p>The encoder processes and compresses the input into a meaningful representation, a fixed length vector. The decoder then takes this meaningful representation and then generates the output 1 token at a time.</p> <p>In <d-cite key="10.5555/2969033.2969173"></d-cite> both the encoder and decoder are RNNs, (or more specifically a variant called LSTM). Let’s take a look at how an RNN works.</p> <h2 id="rnns">RNNs</h2> <p>The basic form of an RNN is the following, in pseudo-code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VanillaRNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inital_state</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_t</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">)</span>
            <span class="n">outs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="n">h_t</span>

</code></pre></div></div> <p>In words, we initialize a “hidden state” or $h_t$ of the RNN to the current state. We loop over every token in the input. For each token we calculate a function called $f$ which takes as input the current token $x_t$ as well as the current state $h_t$. We append the current hidden state for timestep $t$ to the list of outputs <code class="language-plaintext highlighter-rouge">outs</code>.</p> <p>In math, $f$ takes a specific form.</p> \[h_t = f(h_t, x_t; W) = tanh(W_{hh}h_t + W_{xh}x_t + b_h)\] <p>The <strong>hidden state</strong>, $h_t$ is what stores the compressed/meaningful representation of the sequence. Let’s look at how the hidden state is calculated.</p> <p>There are 2 weight matrices involved:</p> <p>\(W_{hh}\in \mathbb{R}^{h \times h}\) \(W_{xh}\in \mathbb{R}^{x \times h}\)</p> <p>as you can see $W_{hh}$ is multiplied with $h_t$, $W_{xh}$ is multiplied with $x_t$ and the 2 resulting vectors are added along with a bias.</p> <p>The resulting vector is then passed through the a function $tanh$ which is just a function that maps $(-\infty, +\infty) \to (-1, 1)$. It looks like this:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/Hyperbolic_Tangent.svg" alt="tanh"></p> <p>that means that the hidden state is a vector $h_t \in \mathbb{R}^h$ where each element is between $(-1, 1$)</p> <h3 id="example">example</h3> <p>Let’s look at a simple example using the sentence “I have socks.” as input.</p> <p>Let’s tokenize the sentence as [“I”, “have”, “socks”, “.”]</p> <p>Let the embedding vector be $x \in \mathbb{R}^2$</p> <p>Let the hidden state $h \in \mathbb{R}^3$.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>


<span class="k">class</span> <span class="nc">VanillaRNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_initial_state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Wxh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Whh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">seq</span><span class="p">):</span>
        <span class="n">h_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_initial_state</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_t</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">x_t</span><span class="p">))</span>
            <span class="n">h_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_f</span><span class="p">(</span><span class="n">h_t</span><span class="p">,</span> <span class="n">x_t</span><span class="p">)</span>
            <span class="n">outs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">h_t</span><span class="p">)</span>
        <span class="n">outs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">outs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outs</span><span class="p">,</span> <span class="n">h_t</span>

    <span class="k">def</span> <span class="nf">_f</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">h_t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nc">Wxh</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="nc">Whh</span><span class="p">(</span><span class="n">h_t</span><span class="p">))</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">torch</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">rnn</span> <span class="o">=</span> <span class="nc">VanillaRNN</span><span class="p">()</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">have</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">socks</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
        <span class="n">outs</span><span class="p">,</span> <span class="n">h_t</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="n">seq</span><span class="o">=</span><span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">have</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">socks</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">]])</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">outs</code>:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[-0.3126,  0.5246,  0.6447],
        [ 0.0277,  0.3934,  0.8409],
        [-0.8055, -0.0275,  0.0972],
        [-0.2049,  0.3322,  0.3161]])
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">h_t</code></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([-0.2049,  0.3322,  0.3161])
</code></pre></div></div> <p><strong>What has happened here?</strong></p> <p>First we split up the sequence “I have socks.” into tokens “I”, “have”, “socks”, “.”</p> <p>Then we mapped each token to a unique number using the variable <code class="language-plaintext highlighter-rouge">vocab</code> to map between token and number.</p> <p>For each token id we looked up an embedding vector. The embedding vector is a fixed length numeric vector and is a way to map each token to a vector of numbers, since the neural network can only take numeric values as input.</p> <p>Mapping the sequence of tokens into a sequence of vectors gives us:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/embedding.png" alt="embedding"></p> <p>We initialized the hidden state \(h_0 = \begin{bmatrix} 0 &amp; 0 &amp; 0 \end{bmatrix}\)</p> <p>then we iterated over each token, and updated the hidden state for each token.</p> <p>In detail:</p> <ol> <li> <p>extract embedding vector for token “I”, update hidden state $h_1$ \(x_1 = \begin{bmatrix}-0.8330 &amp; -0.4121\end{bmatrix}\) \(h_1 = tanh(W_{hh}h_0 + W_{xh}x_1 + b_{hh} + b_{xh})\) \(h_1 = \begin{bmatrix}-0.3126 &amp; 0.5246 &amp; 0.6447\end{bmatrix}\)</p> </li> <li> <p>extract embedding vector for token “have”, update hidden state $h_2$ \(x_2 = \begin{bmatrix}-1.1774 &amp; 0.7259\end{bmatrix}\) \(h_2 = tanh(W_{hh}h_1 + W_{xh}x_2 + b_{hh} + b_{xh})\) \(h_2 = \begin{bmatrix}0.0277 &amp; 0.3934 &amp; 0.8409\end{bmatrix}\)</p> </li> <li> <p>extract embedding vector for token “socks”, update hidden state $h_3$ \(x_3 = \begin{bmatrix}0.5375 &amp; 0.0382\end{bmatrix}\) \(h_3 = tanh(W_{hh}h_2 + W_{xh}x_3 + b_{hh} + b_{xh})\) \(h_3 = \begin{bmatrix}-0.8055 &amp; -0.0275 &amp; 0.0972\end{bmatrix}\)</p> </li> <li> <p>extract embedding vector for token “.”, update hidden state $h_4$ \(x_4 = \begin{bmatrix}-0.6446 &amp; -1.0341\end{bmatrix}\) \(h_4 = tanh(W_{hh}h_3 + W_{xh}x_4 + b_{hh} + b_{xh})\) \(h_4 = \begin{bmatrix}-0.2049 &amp; 0.3322 &amp; 0.3161\end{bmatrix}\)</p> </li> </ol> <p>After all this math, we get a final hidden state $h_4$ and an output $o$ which includes $h_t$ for each timestep $t$</p> \[h_4 = \begin{bmatrix}-0.2049 &amp; 0.3322 &amp; 0.3161\end{bmatrix}\] \[o = \begin{bmatrix}-0.3126 &amp; 0.5246 &amp; 0.6447\\0.0277 &amp; 0.3934 &amp; 0.8409\\-0.8055 &amp; -0.0275 &amp; 0.0972\\-0.2049 &amp; 0.3322 &amp; 0.3161\end{bmatrix}\] <p>What do these numbers represent? The final hidden state $h_4$ represents the RNN’s compressed/useful representation of the sequence. The outputs $o_t$ represent the compressed/useful representation after processing token $t$ and having processed the previous $t-1$ tokens.</p> <p>The hidden state is reset to $0$ each time it processes a new sequence.</p> <h3 id="why-recurrent-neural-network">Why “recurrent” neural network</h3> <p>Why are RNNs called “recurrent” neural networks?</p> <p>The name “recurrent” in “recurrent neural network” comes from the fact that the hidden state $h_{t-1}$ is recursively used to calculate the new hidden state $h_t$</p> <p>If we unpack a single update of $\textcolor{white}{h_t}$, we get:</p> \[\textcolor{white}{h_t = tanh(W_{hh}\textcolor{lime}{h_{t-1}} + W_{xh}x_t + b_{hh} + b_{xh})}\] <p>if we unpack $\textcolor{lime}{h_{t-1}}$ we get:</p> \[\textcolor{lime}{h_{t-1} = tanh(W_{hh}\textcolor{orange}{h_{t-2}} + W_{xh}x_{t-1} + b_{hh} + b_{xh})}\] <p>Plugging this into the previous equation, we get:</p> \[\textcolor{white}{h_t = tanh(W_{hh}(}\textcolor{lime}{tanh(W_{hh}\textcolor{orange}{h_{t-2}} + W_{xh}x_{t-1} + b_{hh} + b_{xh})}\textcolor{white}{) + W_{xh}x_t + b_{hh} + b_{xh})}\] <p>If we unpack this for one more timestep, $\textcolor{orange}{h_{t-2}}$ would be \(\textcolor{orange}{h_{t-2} = tanh(W_{hh}h_{t-3} + W_{xh}x_{t-2} + b_{hh} + b_{xh})}\)</p> <p>Plugging in $\textcolor{orange}{h_{t-2}}$ we get:</p> \[\textcolor{white}{h_t = tanh(W_{hh}(}\textcolor{lime}{tanh(W_{hh}(}\textcolor{orange}{tanh(W_{hh}h_{t-3} + W_{xh}x_{t-2} + b_{hh} + b_{xh})}\textcolor{lime}{) + W_{xh}x_{t-1} + b_{hh} + b_{xh})}\textcolor{white}{) + W_{xh}x_t + b_{hh} + b_{xh})}\] <p>The full recursive equation would be:</p> \[h_t = tanh(W_{hh}(tanh(W_{hh}(...tanh(W_{hh}h_0 + W_{xh}x_1 + b_{hh} + b_{xh})...) + W_{xh}x_{t-1} + b_{hh} + b_{xh})) + W_{xh}x_t + b_{hh} + b_{xh})\] <ul> <li>The calculation of the hidden state $\textcolor{white}{h_t}$ involves the previous $t-1$ hidden states $\textcolor{lime}{h_{t-1}}, \textcolor{orange}{h_{t-2}}, … h_{0}$</li> <li>The hidden state calculation $\textcolor{white}{h_t}$ involves each of the previous $t-1$ tokens in the sequence $\textcolor{lime}{x_{t-1}}, \textcolor{orange}{x_{t-2}}, … x_{0}$. This enables the model to “see” the entire sequence.</li> <li>The weights and biases $W_{hh}$, $W_{xh}$, $b_{hh}$, $b_{xh}$ are used repeatedly when processing the sequence for each hidden state $h_t$</li> <li>There are a lot of matrix-matrix multiplications ($\textcolor{white}{W_{hh}} * \textcolor{lime}{W_{hh}} * \textcolor{orange}{W_{hh}}$). This can lead to very large numbers (exploding gradients) or very small numbers (vanishing gradients)</li> </ul> <p class="box-note"><strong>Note:</strong> This is showing the forward pass of a simple RNN using a single training example. The model would actually be trained on many examples, and the model parameters $W_{hh}$, $W_{xh}$, $b_{hh}$, $b_{xh}$ would be trained using backpropagation.</p> <h3 id="more-complicated-architectures">More complicated architectures</h3> <p>There are various issues with this simple “vanilla RNN” architecture that people have worked on addressing over the years, and there are other improvements to increase complexity and expressiveness.</p> <p>Mentioned above is the issue that because there are many matrix-matrix multiplications, the simple RNN has the issue of vanishing and exploding gradients which prevent the network from learning well. People have worked on addressing this issue by introducing <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" rel="external nofollow noopener" target="_blank">LSTM</a> and <a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.html" rel="external nofollow noopener" target="_blank">GRU</a> and these variants are used in practice instead of the simple RNN. These fix the issue of vanishing gradients; exploding gradients are usually handled by thresholding the gradients if the norm is larger than a certain value.</p> <p>To increase the model expressiveness, you can stack multiple RNNs together where the output from 1 is given as the input to the next. Set <code class="language-plaintext highlighter-rouge">num_layers</code> argument <code class="language-plaintext highlighter-rouge">&gt;1</code> in pytorch to do this.</p> <p>It might also be useful to process a sequence in the forwards direction and in the backwards direction. To do this, you can use 2 RNNs: 1 that processes the sequence forwards and the other backwards. Set <code class="language-plaintext highlighter-rouge">bidirectional=True</code> in pytorch to do this.</p> <h2 id="encoder-decoder-architecture">Encoder-decoder architecture</h2> <p>So far we’ve discussed how an RNN can be used to process the source sequence. What about translation? To do translation, we have to add an additional step.</p> <p>The proposed architecture is the so-called encoder-decoder architecture. The idea is that an RNN processes the input sequence, as we’ve shown above. Then another RNN, the “decoder”, takes the hidden state of the “encoder” RNN as input and tries to predict the correct translated token at each timestep. From <d-cite key="DBLP:conf/emnlp/ChoMGBBSB14"></d-cite>:</p> <blockquote> <p>The proposed neural network architecture, which we will refer to as an RNN Encoder–Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence.</p> </blockquote> <p>Here is a picture of what that would look like for the sequence “I have socks.” -&gt; יש לי גרביים</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/encoder_decoder.png" alt="encoder decoder"></p> <p>You’ll notice a few things from this picture of the encoder-decoder architecture for translation.</p> <ul> <li>The encoder and decoder look similar. They are both separate RNNs that are connected to each other and trained together. Each have separate hidden states.</li> <li>The inputs to each timestep $t$ in the decoder are the <span style="color:rgb(245,66,213)">context vector</span> as well as the <span style="color:rgb(0,103,20)">previous token</span>.</li> <li>The <span style="color:rgb(245,66,213)">context vector</span> is the last hidden state $h_t$ in the encoder (the encoder’s representation after processing the entire input sequence). The same <span style="color:rgb(245,66,213)">context vector</span> is used for each timestep $t$ in the decoder</li> <li>At each step in the decoder we predict the correct token $\hat{y_{t}}$ and compare it with the <span style="color:rgb(0,103,20)">correct token</span> at that timestep</li> </ul> <p class="box-note"><strong>Note:</strong> $\hat{y_{t}}$ is calculated using the <a href="https://en.wikipedia.org/wiki/Softmax_function" rel="external nofollow noopener" target="_blank">softmax</a> function over every possible token in the target vocabulary. For example, if there are 10,000 possible tokens in the target language vocabulary, then $\hat{y_{t}}$ will be of size 10,000. We then calculate $argmax$ over this vector to choose the most confident token at that timestep.</p> <p class="box-note"><strong>Note:</strong> both the encoder and decoder networks are trained end-to-end using backpropagation using the <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html" rel="external nofollow noopener" target="_blank">cross-entropy</a> loss function</p> <p class="box-note"><strong>Note:</strong> At inference time we use the previously predicted token $\hat{y}_{t-1}$ rather than the truly <span style="color:rgb(0,103,20)">correct token</span> at each timestep $t$</p> <h2 id="attention">attention</h2> <p>You might have noticed that the entire input sequence is compressed into what is called the <strong>context vector</strong>. The decoder must rely on all of the meaning of the input sequence being encoded into the context vector in order to produce the translation.</p> <p>In the example sentence</p> <blockquote> <p>I have socks.</p> </blockquote> <p>it might be possible to properly represent this in a fixed length vector.</p> <p>But what if the sentence was</p> <blockquote> <p>I have socks that are thick and made of wool because it is cold outside; it is comfortable to wear around the house while I look outside at the white snow gently falling to the ground.</p> </blockquote> <p>It might be difficult to encode all of the meaning into a fixed length vector representation of numbers so that the decoder can produce a correct translation. It is sort of like the game of “telephone”; the meaning and nuances would be lost in translation.</p> <p>This is the same pattern that <d-cite key="cho-etal-2014-properties"></d-cite> observed, see \ref{mylabel}</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/rnn_sentence_length_bad.png" alt="rnn sentence length bad"></p> <p>The bleu score on the vertical axis is a measure of translation quality, which I’ll discuss later in the article. Notice how as sentence length increases there is an exponential dropoff in translation quality.</p> <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> <p>addressed this in their now famous paper “NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE”.</p> <p>This paper is cited as the paper that introduced attention in neural networks. Attention is the backbone of the transformer architecture which is the architecture used to train LLMs, so it’s sort of a big deal.</p> <p>In the following plot, they compared an encoder-decoder model without attention, as we’ve discussed above, with a model using attention.</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/attention.png" alt="rnn sentence length bad"></p> <p>RNNenc-* is the same model we looked at before. We can see a similar dropoff in translation quality as the sentence length increases.</p> <p>Compare this to RNNsearch-* which is a model with attention. Both RNNsearch models have better performance, and interestingly, the RNNsearch-50 model shows no sign of decrease in performance as the sentence length increases.</p> <h3 id="what-is-attention">what is attention?</h3> <p>Attention, as the name implies is giving the model the ability to pay attention to certain parts of the input.</p> <p>While <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> was the paper to popularize the idea of attention, it was refined and simplified in <d-cite key="10.5555/3295222.3295349"></d-cite>, and the latter is what is typically used. I’ll go over both versions of attention.</p> <h3 id="bahdanau-additive-attention">Bahdanau (additive) attention</h3> <p>Let $s_{i-1}$ be the previous hidden state in the decoder</p> <p>Let $h_j$ be the hidden state in the last layer of the encoder RNN at each timestep $j$</p> <p>Bahdanau attention adds 3 new parameters:</p> <p>\(W_a \in \mathbb{R}^{n \times n}\) \(U_a \in \mathbb{R}^{n \times 2n}\) \(v_a \in \mathbb{R}^{n}\)</p> <p>where $n$ is the hidden state dimensionality in both the encoder and decoder (must be the same) and $U_a$ is n x <em>2n</em> because the encoder was bidirectional in the paper.</p> <p>Then the attention weights $a$ can be calculated by taking as input the decoder previous hidden state $s_{i-1}$ and the hidden state in the last layer of the encoder RNN at each timestep $j$ as:</p> \[a(s_{i-1}, h_j) = {v_a}^Ttanh(W_as_{i-1} + U_ah_j)\] <h4 id="how-does-this-give-the-model-the-ability-to-pay-attention-to-certain-parts-of-the-input">How does this give the model the ability to pay attention to certain parts of the input?</h4> <p>We are taking the current decoder hidden state $s_{i-1}$ when processing token at timestep $i$ and adding it to the hidden state in the encoder at each timestep $j$ $h_j$. That is why this form of attention is referred to as “additive” attention.</p> <p>The result of $a(s_{i-1}, h_j)$ will be $\in \mathbb{R}^{t}$ where $t$ is the number of tokens in the input sequence.</p> <p>It will look like:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/attention_weights.png" alt="attention weights"></p> <p>We then map the numbers to a probability distribution using the $softmax$ function. The attention weights will then look like:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/attention_weights_softmax.png" alt="attention weights"></p> <p>This has the interpretation of “which tokens in the input are most important for outputting the current token in the decoder?”</p> <p>Note that we use a probability distribution rather than a hard cutoff when determining which tokens are important. Some tokens may be less important than others, but have some importance.</p> <h3 id="scaled-dot-product-attention">Scaled dot-product attention</h3> <p>When people today refer to “attention” they are generally referring to the scaled dot-product attention rather than the previously discussed additive attention. There are several reasons for this.</p> <p>You may have noticed that the additive attention is not that straightforward. Why are we adding the decoder hidden state with the encoder hidden state at each timestep?</p> <d-cite key="10.5555/3295222.3295349"></d-cite> <p>in their influential paper “Attention is all you need” reformulate attention to be the following, which is the basis of the transformer architecture.</p> \[attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <p>This is similar to the previously discussed additive attention but with a few key differences.</p> <p>The query $Q$ is the current vector we are asking the question “what should we be paying attention to?”. The keys $K$ are the set of vectors, or matrix, which are the options to pay attention to. The values $V$ is the set of vectors, or matrix, that each key maps to.</p> <p>In the case of translation using the RNN encoder-decoder model, the query $Q$ is the current decoder hidden state of dimension $d_q$. The keys are the encoder outputs which get mapped to the dimension $d_{t \times {d_q}}$ and the values are also the encoder outputs mapped to the dimension $d_{t \times {d_v}}$</p> <p>The idea is that we want to find which <em>keys</em> $K$ in the input the current <em>query</em> $Q$ should attend to. This is computed as the dot product between $Q$ and $K$. The dot product has the theoretical interpretation as the similarity between 2 vectors.</p> <p>We are multiplying the query $Q$ of dimension $q$ with $K^T$ of dimension ${q \times t}$. The resulting vector is of dimension $t$.</p> <p>This is then divided by $\sqrt{d_k}$, and hence the name “scaled” in “scaled dot-product” since we are scaling the dot-product by $\sqrt{d_k}$. The reason for this is given as</p> <blockquote> <p>We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $\sqrt{d_k}$.</p> </blockquote> <p>We then take the $softmax$, resulting in a probability for each timestep $t$.</p> <p>Finally, we multiply this by the matrix $V$. The resulting vector is of size $d_v$ and each element is a weighted sum of importances calculated from the attention weights in the previous step.</p> <h2 id="measuring-performance-of-translation-tasks-bleu-score">Measuring performance of translation tasks: BLEU score</h2> <p>How do we measure the correctness of a machine-translated output?</p> <p>We could hire native speakers and/or professional translators to look at and rate the translation qualities. But looking at and judging the quality would take a very long time and be very costly. It also wouldn’t be reproducible if multiple different algorithms should be compared against a benchmark dataset.</p> <p>Let’s take a stab at a naive translation quality algorithm.</p> <p>Let’s say the correct translation is</p> <blockquote> <p>In my dresser I have socks.</p> </blockquote> <p>but our model outputs</p> <blockquote> <p>I have socks.</p> </blockquote> <p>We can see that the model’s output is partially correct. So maybe if we were to score the correctness of the translation we’d like it be be considered partially correct but not completely correct, and not completely incorrect.</p> <p>If we were to just match up each word with the correct word</p> <p>In -&gt; I</p> <p>my -&gt; have</p> <p>dresser -&gt; socks</p> <p>We would give this a score of 0, since no word matches up. In fact it is exceedingly rare for 2 translations to match a given translation exactly.</p> <p>BLEU score is the standard metric used to evaluate translation models. It was proposed in <d-cite key="10.3115/1073083.1073135"></d-cite>. The goal of the BLEU score is to be automated and to correlate with how well a human would judge the translation quality of a given translation to be.</p> <p>See this interesting figure in the paper in which the authors found the bleu score to correlate with how bilingual humans rated the quality of a translation:</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/bleu_human_correlation.png" alt="bleu human correlation"></p> <p>At a high level, the bleu score computes an average of precisions for $n$-grams up to a certain $n$.</p> <p>Let’s take a look at an example</p> <h3 id="example-1">example</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">evaluate</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">I have socks.</span><span class="sh">"</span><span class="p">]</span>

<span class="n">references</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">In my dresser I have socks.</span><span class="sh">"</span><span class="p">],</span>
<span class="p">]</span>

<span class="n">bleu</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">bleu</span><span class="sh">"</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">bleu</span><span class="p">.</span><span class="nf">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="sh">'</span><span class="s">bleu</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.4723665527410147</span><span class="p">,</span> <span class="sh">'</span><span class="s">precisions</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="sh">'</span><span class="s">brevity_penalty</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.4723665527410147</span><span class="p">,</span> <span class="sh">'</span><span class="s">length_ratio</span><span class="sh">'</span><span class="p">:</span> <span class="mf">0.5714285714285714</span><span class="p">,</span> <span class="sh">'</span><span class="s">translation_length</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="sh">'</span><span class="s">reference_length</span><span class="sh">'</span><span class="p">:</span> <span class="mi">7</span><span class="p">}</span>
</code></pre></div></div> <p>The bleu score for the prediction <code class="language-plaintext highlighter-rouge">I have socks.</code> when the reference translation is <code class="language-plaintext highlighter-rouge">In my dresser I have socks.</code> is $0.47$. The bleu score ranges between $[0, 1]$ and so a bleu score of $0.47$ is decent but not perfect.</p> <p>Let’s break down the calculation further:</p> <p>$1$-grams</p> <ul> <li>Prediction unigrams: {I, have, socks, .}</li> <li>Reference unigrams: {In, my, dresser, I, have, socks, .}</li> <li>Total unigrams in prediction: 4</li> <li>Unigram precision: $p_1$ = $\frac{4}{4} = 1.0$</li> </ul> <p>$2$-grams</p> <ul> <li>Prediction bigrams: {I have, have socks, socks .}</li> <li>Reference bigrams: {In my, my dresser, dresser I, I have, have socks, socks .}</li> <li>Total bigrams in prediction: 3</li> <li>Bigram precision: $p_2$ = $\frac{3}{3} = 1.0$</li> </ul> <p>$3$-grams</p> <ul> <li>Prediction trigrams: {I have socks, have socks .}</li> <li>Reference trigrams: {In my dresser, my dresser I, dresser I have, I have socks, have socks .}</li> <li>Total trigrams in prediction: 2</li> <li>Trigram precision: $p_3$ = $\frac{2}{2} = 1.0$</li> </ul> <p>$4$-grams</p> <ul> <li>Prediction 4-grams: {I have socks .}</li> <li>Reference 4-grams: {In my dresser I, my dresser I have, dresser I have socks, I have socks .}</li> <li>Total 4-grams in prediction: 1</li> <li>4-gram precision: $p_4$ = $\frac{1}{1} = 1.0$</li> </ul> <p>So in this case all the $n$-gram precisions up to $4$ are $1.0$. But the translation is not perfect, so how do we penalize it?</p> <p>A brevity penalty is calculated. The way it is calculated is outside of the scope of this article. See the <d-cite key="10.3115/1073083.1073135"></d-cite> for the equation. In this case the brevity penalty is $0.47$. The brevity penalty is multiplied with the $n$-gram precisions to get a final bleu score of $0.47$.</p> <h3 id="bleu-score-problems">Bleu score problems</h3> <p>However, the bleu score is not a perfect metric.</p> <p>Implicit in the bleu score calculation is that the text has been tokenized. For example, <code class="language-plaintext highlighter-rouge">I have socks.</code> is tokenized as <code class="language-plaintext highlighter-rouge">[I, have, socks.]</code>. This is not the only way to tokenize, as we’ll see later. this could lead to differences in bleu score calculation across papers.</p> <p>It does not take into consideration semantics. Let’s say the reference translation is <code class="language-plaintext highlighter-rouge">I have a hamburger in my bag</code>. Candidate translation 1 is <code class="language-plaintext highlighter-rouge">I have a sandwhich in my bag</code>, and candidate translation 2 is <code class="language-plaintext highlighter-rouge">I have a dolphin in my bag.</code> They would both have the same bleu score, while we can see that candidate 2 is nonsense while candidate 1 is close.</p> <p>It was designed for English translations. Translations in other languages may not have the same “similar n-gram matches correlates with human-judgement of translation quality” property.</p> <p>If one of the n-gram matches is 0, the bleu score is undefined since $log(0)$ is undefined. It is usually set to 0 in this case. For example, the translation <code class="language-plaintext highlighter-rouge">I have socks.</code> with reference <code class="language-plaintext highlighter-rouge">I have socks in my dresser.</code> would have a bleu score of 0 since the $4$-gram has precision of 0. This is not a good property since it is clearly a close translation. This can be fixed by adding 1 to all n-gram precisions, but it’s not clear whether papers use this approach.</p> <p>See <a href="https://huggingface.co/spaces/evaluate-metric/bleu" rel="external nofollow noopener" target="_blank">this page</a> for more discussion of limitations.</p> <p>Regardless, bleu score is the standard for evaluating machine translations in an automated way.</p> <h1 id="experiments-and-results">Experiments and results</h1> <p>Now I’ll actually implement both the RNN encoder-decoder model from <d-cite key="10.5555/2969033.2969173"></d-cite> and the RNN encoder-decoder model with attention from <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, evaluate it on the French to English translation task of the <d-cite key="bojar-etal-2014-findings"></d-cite> dataset, and compare results to the papers.</p> <h2 id="dataset">Dataset</h2> <p>In <d-cite key="10.5555/2969033.2969173"></d-cite> and <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, they use the <d-cite key="bojar-etal-2014-findings"></d-cite> (Workshop on Machine Translation) English-French parallel corpora dataset which contains the following:</p> <ul> <li>Europarl <ul> <li>Professional translations of European parliment</li> </ul> </li> <li>News Commentary <ul> <li>Translations of news articles</li> </ul> </li> <li>UN <ul> <li>Translations from the UN meetings</li> </ul> </li> <li>Crawled corpora <ul> <li>Presumably random text on the web translated into different languages</li> </ul> </li> </ul> <p>The breakdown on the number of reported tokens per each dataset is:</p> <table> <thead> <tr> <th style="text-align: left">Dataset</th> <th style="text-align: left">Num. words</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Europarl</td> <td style="text-align: left">61M</td> </tr> <tr> <td style="text-align: left">News Commentary</td> <td style="text-align: left">5.5M</td> </tr> <tr> <td style="text-align: left">UN</td> <td style="text-align: left">421M</td> </tr> <tr> <td style="text-align: left">Crawled corpora</td> <td style="text-align: left">362.5M</td> </tr> </tbody> </table> <p>They use a technique (Cho et al, 2014) to filter down to a total of <strong>348M</strong> words for training. I did not follow this selection technique.</p> <p>They use a test set of <em>3003</em> sentences for testing.</p> <p>Here’s an example from the <strong>Europarl dataset</strong>:</p> <p>French:</p> <blockquote> <p>Même si cet idéal est encore loin d’être atteint, je voudrais exprimer la satisfaction de l’Union européenne de voir l’élection du Belarus au Conseil des droits de l’homme empêchée avec succès au mois de mai.</p> </blockquote> <p>English:</p> <blockquote> <p>Although this ideal is still far from having been achieved, I should like to express the satisfaction of the European Union that the election of Belarus to the Human Rights Council was successfully thwarted in May.</p> </blockquote> <p>and from the <strong>News Commentary dataset</strong>:</p> <p>French:</p> <blockquote> <p>Si l’on s’inquiète de cette possibilité, l’or pourrait en effet être la couverture la plus fiable.</p> </blockquote> <p>English:</p> <blockquote> <p>And if you are really worried about that, gold might indeed be the most reliable hedge.</p> </blockquote> <h2 id="model">Model</h2> <p>I used an encoder-decoder model using a GRU in both the encoder and decoder. The encoder is bidirectional. The hidden state size in the forward and backward directions of the encoder is $1000$ and in the decoder is $1000$. The embedding dimensionality in both the encoder and decoder is $1000$. Both the encoder and decoder have $4$ layers. For attention, I used the scaled dot-product attention with $d_v$ equal to $1000$. The source vocab size and target vocab size were set to $30,000$. The models with attention and without attention contains ~$189M$ trainable params.</p> <p>For comparison, <d-cite key="10.5555/2969033.2969173"></d-cite> used bidirectional LSTM instead of GRU. The vocab sizes were much larger at $160,000$ for the source and $80,000$ for the target. Their model contain $384M$ params. To the best of my knowledge the attention model implemented is approximately equal to <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, but they leave out some implementation details, so I cannot say for sure.</p> <h2 id="training-details">Training details</h2> <ul> <li>I used 3 nvidia V100 GPUs to train both the model with attention and model without attention.</li> <li>Both models were trained for up to 3 epochs, or 72 hours, whichever came first. The model with attention completed ~3 epochs in 72 hours, while the model without attention completed 3 epochs.</li> <li>$AdamW$ optimizer using cosine annealing with warmup learning rate scheduler was used. Warmup number of steps is $2000$, min learning rate is $6e^{-5}$ and max learning rate is $6e^{-4}$</li> <li>Cross entropy loss was used between the output sequence of tokens with highest probabilty and the target.</li> </ul> <h2 id="results">Results</h2> <p>Results are reported on the WMT’14 test set which contained 3003 english-french pairs.</p> <table> <thead> <tr> <th style="text-align: left">Model</th> <th style="text-align: left">Average BLEU score</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">With attention</td> <td style="text-align: left">0.297</td> </tr> <tr> <td style="text-align: left">Without attention</td> <td style="text-align: left">0.261</td> </tr> </tbody> </table> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/attention_vs_no_attention_bleu.png" alt="attention vs no attention bleu"></p> <p>While the model with attention has better overall performance than the model without, we are not able to reproduce a decrease in performance as the number of input tokens increases. <d-cite key="10.5555/2969033.2969173"></d-cite> also showed that their model without attention is robust to an increase in sentence length even without attention.</p> <h2 id="random-examples">Random examples</h2> <p>Let’s look at a few randomly chosen translations from the test set.</p> <table> <thead> <tr> <th style="text-align: left">Input</th> <th style="text-align: left">With attention Prediction</th> <th style="text-align: left">Without attention Prediction</th> <th style="text-align: left">Target</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">California planners are looking to the system as they devise strategies to meet the goals laid out in the state’s ambitious global warming laws.</td> <td style="text-align: left">Les planificateurs de la Californie se tournent vers le système lorsqu’ils élaborent des stratégies pour atteindre les objectifs énoncés dans les lois ambitieuses de l’État sur le réchauffement planétaire.</td> <td style="text-align: left">Les planificateurs californiens se tournent vers le système pour mettre au point des stratégies visant à atteindre les objectifs énoncés dans la législation ambitieuse du réchauffement de la planète.</td> <td style="text-align: left">Les planificateurs de Californie s’intéressent au système puisqu’ils élaborent des stratégies pour atteindre les objectifs fixés dans les lois ambitieuses de l’État sur le réchauffement climatique.</td> </tr> </tbody> </table> <p>While I don’t speak French, plugging in both predictions into Google Translate produces:</p> <p><strong>With attention</strong></p> <blockquote> <p>California planners are turning to the system as they develop strategies to meet the goals set out in the state’s ambitious global warming laws.</p> </blockquote> <p><strong>Without attention</strong></p> <blockquote> <p>California planners are looking to the system to develop strategies to meet goals set out in ambitious global warming legislation.</p> </blockquote> <p>Both are close, but the translation with attention is better as it produces <ins>the goals</ins> rather than <ins>goals</ins> and also the translation without attention misses the important phrase <ins>the state’s</ins>. Also the word <em>legislation</em> is awkward and <em>laws</em> is better.</p> <table> <thead> <tr> <th style="text-align: left">Input</th> <th style="text-align: left">With attention Prediction</th> <th style="text-align: left">Without attention Prediction</th> <th style="text-align: left">Target</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">“Despite losing in its attempt to acquire the patents-in-suit at auction, Google has infringed and continues to infringe,” the lawsuit said.</td> <td style="text-align: left">“Malgré la perte dans sa tentative d’acquérir les brevets en cours d’enchère, Google a enfreint et continue de violer”, a déclaré la poursuite.</td> <td style="text-align: left">“En dépit de sa perte dans l’acquisition des brevets en cours aux enchères, Google a enfreint et continue de violer”, a-t-il poursuivi.</td> <td style="text-align: left">« Bien qu’ayant échoué dans sa tentative d’acquérir les brevets en cause au cours des enchères, Google a violé et continue à violer lesdits brevets », ont indiqué les conclusions du procès.</td> </tr> </tbody> </table> <p><strong>With attention</strong></p> <blockquote> <p>“Despite losing its bid to acquire the patents in the auction, Google has infringed and continues to infringe,” the suit said.</p> </blockquote> <p><strong>Without attention</strong></p> <blockquote> <p>“Despite its loss in the pending patent auction, Google has infringed and continues to infringe,” he continued.</p> </blockquote> <p>We see a similar pattern as before. The model with attention pretty much gets it right, while the model without attention adds the word <ins>pending</ins> to <em>patent auction</em> which is incorrect, and ends with <ins>he continued</ins> which is incorrect and should be <em>the lawsuit said</em>.</p> <table> <thead> <tr> <th style="text-align: left">Input</th> <th style="text-align: left">With attention Prediction</th> <th style="text-align: left">Without attention Prediction</th> <th style="text-align: left">Target</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">He also said that a woman and child had been killed in fighting the previous evening and had been buried.</td> <td style="text-align: left">Il a également déclaré qu’une femme et un enfant avaient été tués dans les combats de la soirée précédente et avaient été enterrés.</td> <td style="text-align: left">Il a également déclaré qu’une femme et un enfant avaient été tués au cours de la nuit précédente et avaient été enterrés.</td> <td style="text-align: left">Selon lui, une femme et son enfant ont été tués par les combats de la veille et ont été enterrées.</td> </tr> </tbody> </table> <p><strong>With attention</strong></p> <blockquote> <p>He also said that a woman and a child had been killed in the fighting the previous evening and had been buried.</p> </blockquote> <p><strong>Without attention</strong></p> <blockquote> <p>He also said that a woman and a child had been killed during the previous night and had been buried.</p> </blockquote> <p>The model with attention gets it right while the model without attention misses the important phrase <ins>in the fighting</ins> and adds the slightly awkward <ins>during</ins>.</p> <h2 id="failure-mode-examples">Failure mode examples</h2> <p>It’s useful to look at examples where the model performs poorly.</p> <table> <thead> <tr> <th style="text-align: left">Input</th> <th style="text-align: left">With attention Prediction</th> <th style="text-align: left">Without attention Prediction</th> <th style="text-align: left">Target</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">I should say that I am rather sociable and have a job with responsibilities. I laugh and sing, I go out, I’m a grandmother, I have lots of friends, but my heart and my soul belong to him. I never talk about him except with my son, and I never go to the cemetery.</td> <td style="text-align: left">Je dois dire que je suis plutôt sociable et j’ai un travail avec des responsabilités.</td> <td style="text-align: left">Je dois dire que je suis assez sociable et qu’il est un homme qui a des devoirs, je rentre et j’essaie de chanter, je suis une grand-mère, j’ai beaucoup d’amis, mais mon cœur et moi-même l’âme, je n’en parle qu’à mon fils, mais</td> <td style="text-align: left">Je tiens à préciser que je suis sociable avec un métier à responsabilité, je ris et je chante, je sors, je suis grand-mère, j’ai plein d’amis, mais mon cœur et mon âme lui appartiennent et je ne parle jamais de lui sauf avec mon fils, et je ne vais jamais au cimetière.</td> </tr> </tbody> </table> <p>Interesting, it looks like the model is having trouble because there are multiple sentences separated with “.”, which is not common in the dataset. Each sentence is supposed to be split. The model has trouble with this and stops at the first period.</p> <p>This is a failure mode that would definitely need to be worked on, possibly by splitting on period and translating each sentence separately.</p> <h2 id="impact-of-beam-search">Impact of beam search</h2> <p>Later, in the section <a href="#sequence-generation">sequence generation</a> I’ll describe different sequence generation methods including Beam search and greedy sequence generation. The above results are using beam search using a beam width of 10. Briefly, greedy will choose the token with highest probability at each decoding step, while beam search will keep all partial sequences and will maintain a list of top performing “beams” which can replaced if a more likely sequence is found at any iteration of beam search. Greedy has the downside that it might choose a token that is locally optimal but which produces a suboptimal sequence but has the upside that it is significantly faster than beam search. I’ll describe what this means in more detail in the later section.</p> <p>I’ll compare greedy vs. beam search results</p> <table> <thead> <tr> <th style="text-align: left">Sequence generation method</th> <th style="text-align: left">Average Bleu score</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Greedy</td> <td style="text-align: left">0.372</td> </tr> <tr> <td style="text-align: left">Beam search with $beam width = 10$</td> <td style="text-align: left">0.387</td> </tr> </tbody> </table> <p>So beam search slightly improves overall performance, but at the cost of taking significantly longer to run.</p> <p>We can see from the figure below that beam search in general improves performance compared to greedy, but also produces an translation of equivalent quality to greedy more generally and sometimes worse.</p> <p><img src="/assets/img/2024-06-22-sequence_to_sequence_translation/beam_search_diff.png" alt="bleu diff"></p> <h1 id="implementation-details">Implementation details</h1> <h2 id="tokenization"> <a name="tokenization"></a>Tokenization</h2> <p>In <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite>, tokenization was treated as an after-thought. They say</p> <blockquote> <p>After a usual tokenization, …</p> </blockquote> <p>However I soon realized that tokenization is an important topic and can influence the model in many ways. Curious what SOTA LLMs use, I found that it is more common to use an algorithm called Byte-pair encoding algorithm, for example as discussed in <d-cite key="radford2019language"></d-cite>.</p> <h3 id="byte-pair-encoding-algorithm">Byte-pair encoding algorithm</h3> <p>As mentioned previously, tokenization is the way by which we feed the inputs to the model. Unfortunately, there is no way currently to feed the inputs directly to the model. The inputs must be chunked into discrete <strong>tokens</strong>.</p> <p>Traditionally, inputs might be tokenized by breaking up into words or characters.</p> <p>For example in the sentence</p> <blockquote> <p>I have socks.</p> </blockquote> <p>it might be tokenized as <code class="language-plaintext highlighter-rouge">['I', ' ', 'have', ' ', 'socks', '.']</code></p> <p>but it could also be tokenized as <code class="language-plaintext highlighter-rouge">['I', ' ', 'h', 'a', 'v', 'e', ' ', 's', 'o', 'c', 'k', 's', '.']</code></p> <p>The “vocabulary” is then the set of possible tokens. One problem is that not every word encountered might be represented in the vocabulary. Traditionally, rare words or given the token “UNKOWN” so as to limit the vocabulary, and to handle tokens not seen in the training set. However this is not great; we’d like the model to be able to handle anything without defaulting to an UNKNOWN token.</p> <p>Splitting on words is not great as that introduces a bias. In deep learning we like the model to learn the raw signal without human intervention and splitting on words is somewhat of a human intervention. Splitting on characters is better, but introduces other problems as discussed in <d-cite key="radford2019language"></d-cite>.</p> <p>One idea is to feed the model UTF-8 bytes directly. However, for each character, 1-4 bytes are used in UTF-8. We would have a vocabulary of size 256 ($2^8$ possible tokens can be modeled with a single byte). In this case the input size would explode in some cases since for each character we are using 1-4 tokens. For english tokens just a single byte is used but for characters in other languages and special tokens such as math, up to 4 bytes are used. This is a problem computationally, and so we would like the inputs to be somewhere on the spectrum of “word-level tokenization”, “character-level tokenization”, and “UTF-8 byte level representation”.</p> <d-cite key="radford2019language"></d-cite> <p>(GPT-2 paper) adopted the BPE (byte-pair encoding) algorithm, which allows for a middle ground between these representations, and solves the issue of being able to model any possible character.</p> <p>The BPE algorithm in short converts a “training set”, which is possibly a sample of text used to train for the problem we’re working on, or a completely different set of text, to UTF-8 bytes. As mentioned, this is a vocabulary of size 256. We want to expand the vocabulary.</p> <p>Then, the most common pairs of bytes are merged. These merges get added to the vocabulary.</p> <p>This continues until the vocabulary gets to a certain size, which is a hyperparameter.</p> <p>Note that this solves the problems mentioned above. We are using a byte-level representation, but for common sequences of bytes, such as common words, we are representing those as a single token. For uncommon characters, or characters not even in the training set, those are left as individual bytes.</p> <p class="box-note"><strong>Side note:</strong> The BPE algorithm was originally developed as a <a href="https://web.archive.org/web/20160326130908/http://www.csse.monash.edu.au/cluster/RJK/Compress/problem.html" rel="external nofollow noopener" target="_blank">compression algorithm</a> in 1994.</p> <h3 id="implementation-of-bpe">Implementation of BPE</h3> <p>OpenAI has released their <a href="https://github.com/openai/tiktoken" rel="external nofollow noopener" target="_blank">BPE algorithm</a> but it can only be used for inference.</p> <p>The <a href="https://github.com/google/sentencepiece" rel="external nofollow noopener" target="_blank">sentencepiece algorithm</a> is similar to the above implementation, and was used for Meta’s LLama 2. It has a very efficient training implementation, so this is what I used.</p> <p>See the excellent overview of BPE and sentencepiece by Andrej Karpathy</p> <iframe width="560" height="315" src="https://www.youtube.com/embed/zduSFxRajkE?si=M2xaub0-iBADHwnY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> <h2 id="using-multiple-gpus">Using multiple GPUs</h2> <p>To train on multiple GPUs, the <code class="language-plaintext highlighter-rouge">torch.nn.parallel.DistributedDataParallel</code> package in pytorch was used. Since it was the first time using it, I’ll explain briefly how it works.</p> <p>At a high level, the model is duplicated across the GPUs, each running in a separate process. When the backward pass is completed, the gradients are synced with a master process. The updated gradients then sync with the other processes.</p> <p>To use <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>, we just need to wrap the pytorch model in <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[...])</span>
</code></pre></div></div> <p>To make use of DDP, we can use the <code class="language-plaintext highlighter-rouge">torchrun</code> command line utility in pytorch. It assists in launching multiple processes and setting environment variables.</p> <p>Important envionrment variables are <code class="language-plaintext highlighter-rouge">RANK</code> and <code class="language-plaintext highlighter-rouge">LOCAL_RANK</code>. <code class="language-plaintext highlighter-rouge">RANK</code> is the global identifier for the process, and <code class="language-plaintext highlighter-rouge">LOCAL_RANK</code> is the local identifier for a process. <code class="language-plaintext highlighter-rouge">torchrun</code> can launch processes across machines, in which case these will be different, but on the same machine they will be the same. <code class="language-plaintext highlighter-rouge">device_ids</code> should be the <code class="language-plaintext highlighter-rouge">LOCAL_RANK</code>.</p> <p class="box-note"><strong>Note:</strong> Only log and check validation performance from the master process (the one where <code class="language-plaintext highlighter-rouge">RANK == 0</code>)!</p> <h2 id="padding">Padding</h2> <p>During development of the model I experienced a strange issue that took all day to debug. I’ll first describe why padding is needed, the issue I encountered, and then the solution.</p> <p>When training we construct minibatches to parallelize the computation. We must pass a single tensor to the model, and so all sequences must be made to be the same length. This means that if the sequences are different lengths, then we need to add a special token to any sequence which is shorter than the longest sequence.</p> <p>Let’s say we had this minibatch: <img src="/assets/img/2024-06-22-sequence_to_sequence_translation/padding.png" alt="padding"></p> <p>The 3 padding tokens in the 1st sequence are required because the 1st sequence is shorter than the 2nd sequence.</p> <h3 id="the-issue">the issue</h3> <p>What I encountered is that when I passed the 1st sequence as is with the 3 padding tokens through the encoder-decoder, the output translation was, let’s say “J’ai des chaussettes.” But when I removed the padding tokens, and just passed the sequence by itself without padding tokens, I got a totally different translation.</p> <p>This to me was a bad sign. The model should not be sensitive to the padding tokens; they should be ignored. Their only purpose should be to make the sequences in the minibatch the same length and shouldn’t have any meaning.</p> <h3 id="the-solution">the solution</h3> <p>Padding affects the model in a few ways.</p> <h4 id="encoding">encoding</h4> <p>The 1st way is when we encode the input sequences in the minibatch, the RNN must iterate through each token in the sequence, including the pad tokens. We would like for the encoder to learn that the pad tokens are irrelevent, which is what I thought would happen with enough data. But clearly this was not the case. How can we force the encoder to ignore the pad tokens?</p> <p>It turns out in pytorch there is function to force RNNs to ignore the pad tokens.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">packed_embedded</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="nf">pack_padded_sequence</span><span class="p">(</span>
    <span class="nb">input</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span>
    <span class="n">lengths</span><span class="o">=</span><span class="n">input_lengths</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">enforce_sorted</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>
</code></pre></div></div> <p>Given the tensor with the pad tokens, and the lengths of each sequence in the tensor, <code class="language-plaintext highlighter-rouge">torch.nn.utils.rnn.pack_padded_sequence</code> will construct a <code class="language-plaintext highlighter-rouge">PackedSequence</code> object which any RNN module in pytorch knows how to take as input. The RNN will know to ignore the pad tokens.</p> <p>#### loss function</p> <p>In the cross-entropy loss we are comparing the predicted token with the actual token for a minibatch of predicted and actual sequences, e.g.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span>
    <span class="n">decoder_outputs</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span>
    <span class="n">target_tensor</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p>What happens if the sequences are padded with a bunch of pad tokens? If we evaluate the loss where a significant number of <code class="language-plaintext highlighter-rouge">decoder_outputs</code> are pad tokens and <code class="language-plaintext highlighter-rouge">target_tensor</code> are pad tokens, then the loss will be biased towards the pad tokens. We want the loss to reflect the actual tokens, and not the pad tokens. It turns out that <code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code> supports <code class="language-plaintext highlighter-rouge">ignore_index</code> which will ignore a specific index in the loss calculation; in this case it should be set to the <code class="language-plaintext highlighter-rouge">&lt;pad&gt;</code> token.</p> <h4 id="attention-1">attention</h4> <p>The other place where the pad token needs to be ignored is when calculating the attention weights. We don’t want the model to pay attention to the pad token.</p> <p>If we look again at the equation for attention:</p> \[attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <p>We want $V$ to be 0 where the padding tokens are. To do that we just need to set the scores $QK^T$ to $-{inf}$ where the padding tokens are.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">Dq</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p>where $mask$ is $1$ where the pad tokens are.</p> <h4 id="does-it-work">does it work?</h4> <p>Hopefully all this effort to ignore the pad token works and the model is insensitive to the number of pad tokens, which wasn’t the case when I didn’t ignore the pad token in the RNN encoder and in the attention layer.</p> <p>In the following example we randomly sample a test example, check the output with no padding and compare it with the output with a randomly chosen number of pad tokens appended.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_padding</span><span class="p">():</span>
	<span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span> <span class="o">=</span> <span class="nf">construct_model_attention</span><span class="p">()</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
		<span class="n">num_pad_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">50</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
		<span class="n">dset_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">test_dset</span><span class="p">))[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
		
		<span class="nb">input</span> <span class="o">=</span> <span class="n">test_dset</span><span class="p">[</span><span class="n">dset_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
		<span class="n">input_with_pad_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">num_pad_tokens</span><span class="p">)</span> <span class="o">*</span> <span class="n">source_tokenizer</span><span class="p">.</span><span class="n">processor</span><span class="p">.</span><span class="nf">pad_id</span><span class="p">()).</span><span class="nf">long</span><span class="p">()])</span>
		
		<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">input: </span><span class="si">{</span><span class="n">source_tokenizer</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
		
		<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">decoded_ids_no_padding</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span>
			<span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
			<span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span>
			<span class="n">input_tensor</span><span class="o">=</span><span class="nb">input</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
			<span class="n">input_lengths</span><span class="o">=</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)]</span>
		<span class="p">)</span>
		
		<span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">decoded_ids_padding</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span>
			<span class="n">encoder</span><span class="o">=</span><span class="n">encoder</span><span class="p">,</span>
			<span class="n">decoder</span><span class="o">=</span><span class="n">decoder</span><span class="p">,</span>
			<span class="n">input_tensor</span><span class="o">=</span><span class="n">input_with_pad_tokens</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
			<span class="n">input_lengths</span><span class="o">=</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">input_with_pad_tokens</span><span class="p">)]</span>
		<span class="p">)</span>
		
		<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">output with no pad tokens appended to input == output with </span><span class="si">{</span><span class="n">num_pad_tokens</span><span class="si">}</span><span class="s"> pad tokens appended to input</span><span class="sh">'</span><span class="p">,</span> <span class="p">(</span><span class="n">decoded_ids_padding</span> <span class="o">==</span> <span class="n">decoded_ids_padding</span><span class="p">).</span><span class="nf">all</span><span class="p">().</span><span class="nf">item</span><span class="p">())</span>
		
		<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">=</span><span class="sh">'</span><span class="o">*</span><span class="mi">11</span><span class="p">)</span>
		
		
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">check_padding</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input: In a police interview he said he ran an office at his home address as well as work place and clients would call at his house on legal business.
output with no pad tokens appended to input == output with 1 pad tokens appended to input True
===========
input: A US official confirmed that there had been an "Israeli strike", but did not give details of the target.
output with no pad tokens appended to input == output with 33 pad tokens appended to input True
===========
input: As a result, the seatbacks fail to comply with federal auto safety standards on head restraints.
output with no pad tokens appended to input == output with 9 pad tokens appended to input True
===========
input: Amid the uproar, Murphy said his office is arranging the congressional trip, expected to take place this year, and hopes the delegation will include members of both parties and both chambers.
output with no pad tokens appended to input == output with 18 pad tokens appended to input True
===========
input: There is debate about what constitutes the actual onset of puberty, but it is considered "precocious" when breast enlargement is accompanied by a growth spurt before age 8.
output with no pad tokens appended to input == output with 44 pad tokens appended to input True
===========
</code></pre></div></div> <p>Great, the model seems to be ignoring the pad tokens!</p> <h2 id="sequence-generation">Sequence generation</h2> <p>When we’re producing the output sequences it turns out that there are multiple ways to go about it and some are better than others.</p> <h3 id="greedy-search">greedy search</h3> <p>The simplest is called “greedy search” and involves just selecting the token with the highest predicted probability of being the correct token at each timestep.</p> <p>I thought this would be sufficient, as the model is being optimized to learn which is the correct token at each timestep $t$ given the previous $t-1$ tokens.</p> <p>However, it turns out that this can produce suboptimal, awkwardly phrased, or strange outputs. It can select a bad token at timestep $t$ and then this creates a bad sequence going forward.</p> <h3 id="beam-search">beam search</h3> <p>A solution to this is to use “beam search”. <d-cite key="10.5555/2969033.2969173"></d-cite> and <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> both use beam search to produce the output sequences. At first I skipped over this detail as I thought it might have been a relic of the past, but it turns out to be an important detail.</p> <p>At each timestep $t$, beam search will choose the $B$ most likely next tokens, as well as store the following things:</p> <ul> <li>the decoder hidden state at timestep $t$</li> <li>the generated sequence at timestep $t$</li> <li>The $log-likelihood$ of the sequence</li> </ul> <p>We start by choosing $B$ most likely tokens to start the sequence. Then for each of these sequences, we select the $B$ most likely next tokens, producing $B^2$ possible sequences. From this list of possible sequences, we choose the $B$ most promising by comparing the $log-likelihood$ of each sequence and choosing the $B$ sequences with the lowest $log-likelihood$. We complete the search once all $B$ beams terminate with an <code class="language-plaintext highlighter-rouge">end of sentence</code> token or when we’ve done a certain number of iterations of search, whichever comes first.</p> <h4 id="beam-search-example">beam search example</h4> <p>As an example, let’s consider the following input:</p> <blockquote> <p>The theatre director immediately began an evacuation procedure and called the fire brigade to check out a suspicious smell.</p> </blockquote> <p>This has the ground truth french translation:</p> <blockquote> <p>Immédiatement, le directeur de l’établissement a fait procéder à l’évacuation de la salle et a prévenu les pompiers pour une odeur suspecte.</p> </blockquote> <p>The greedy search prediction is:</p> <blockquote> <p>Le directeur du théâtre a immédiatement entrepris une procédure d’évacuation et a appelé la brigade de pompiers pour vérifier une odeur suspecte.</p> </blockquote> <p>Let’s see what beam search produces.</p> <p>For this example I am using a $beam-size$ $B$ of $2$.</p> <p>Beam search starts off predicting the 1st token:</p> <p><strong>iteration 1</strong></p> <table> <thead> <tr> <th style="text-align: left">Sequence</th> <th style="text-align: left">$log-likelihood$</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"> </td> <td style="text-align: left">0.0</td> </tr> <tr> <td style="text-align: left">’</td> <td style="text-align: left">-19.966</td> </tr> </tbody> </table> <p>It starts off predicting start of sentence and an apostrophe token.</p> <p>Then, for each of these beams, it predicts $B$ next tokens:</p> <p><strong>iteration 2</strong></p> <table> <thead> <tr> <th style="text-align: left">Sequence</th> <th style="text-align: left">$log-likelihood$</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Le</td> <td style="text-align: left">-0.387</td> </tr> <tr> <td style="text-align: left">La</td> <td style="text-align: left">-3.325</td> </tr> <tr> <td style="text-align: left">’ Le</td> <td style="text-align: left">-20.716</td> </tr> <tr> <td style="text-align: left">’ La</td> <td style="text-align: left">-22.956</td> </tr> </tbody> </table> <p>For each of the previous 2 beams, we search for the next token. <em>Le</em> is the most likely, while <em>La</em> is second most likely. We select these 2 beams and continue expanding both.</p> <p><strong>iteration 3</strong> | Sequence | $log-likelihood$ | | :—— |:— | |Le directeur| -0.560| | Le chef | -4.279 | | La directrice | -3.885 | | La direction| -5.589 |</p> <p>Let’s see what happens next.</p> <p><strong>iteration 4</strong> | Sequence | $log-likelihood$ | | :—— |:— | |Le directeur du| -1.102| | Le directeur de | -1.667 | | La directrice du | -4.574 | | La directrice de| -5.007 |</p> <p><strong>iteration 5</strong> | Sequence | $log-likelihood$ | | :—— |:— | |Le directeur du théâtre| -1.267| | Le directeur du secteur| -5.419 | | Le directeur de théâtre |-2.450 | | Le directeur de la| -2.565 |</p> <p>We can see how beam search is trying out different possibilities for the next token and adjustment the likelihood after expanding each of the most likely $B$ beams $B$ times.</p> <p>It continues like this for 28 iterations. The final list of beam candidates are:</p> <table> <thead> <tr> <th style="text-align: left">Sequence</th> <th style="text-align: left">$log-likelihood$</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Le directeur du théâtre a immédiatement entrepris une procédure d’évacuation et a appelé la brigade d’incendie pour vérifier une odeur suspecte.</td> <td style="text-align: left">-12.367</td> </tr> <tr> <td style="text-align: left">Le directeur du théâtre a immédiatement entrepris une procédure d’évacuation et a appelé la brigade d’incendie pour vérifier une odeur suspecte de</td> <td style="text-align: left">-19.076</td> </tr> <tr> <td style="text-align: left">Le directeur du théâtre a immédiatement entrepris une procédure d’évacuation et a appelé la brigade d’incendie à vérifier une odeur suspecte.</td> <td style="text-align: left">-12.813</td> </tr> <tr> <td style="text-align: left">Le directeur du théâtre a immédiatement entrepris une procédure d’évacuation et a appelé la brigade d’incendie à vérifier une odeur suspecte de</td> <td style="text-align: left">-12.813</td> </tr> </tbody> </table> <p>We can see that 2 of the sequences are incomplete, but because $B=2$ of the sequences are complete, beam search terminates. This is a bias of beam search that it prefers shorter sequences.</p> <h2 id="handling-unknown-tokens">Handling unknown tokens</h2> <p>In previous attempts at language modeling, a fixed size vocabulary would be constructed and used for training. What happens when the model sees an unknown token at test time? What should the model do? The model was typically just output a special <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> token, standing for unknown. But this is not useful. We don’t want to see that. When using chatgpt for example, we don’t want to see <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> in the output, but what if a user enters something it has never seen before?</p> <p>We mentioned before in the section on <a href="#tokenization">tokenization</a> that the initial vocabulary when using the <code class="language-plaintext highlighter-rouge">sentencepiece</code> algorithm with <code class="language-plaintext highlighter-rouge">byte_fallback=True</code> is that the initial vocabulary contains all UTF-8 bytes. When an unexpected token is encountered in this scenario, the sentencepiece algorithm will break down the token into its UTF-8 bytes, and since each UTF-8 byte is part of the vocabulary, the model will not need to output the <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code> token.</p> <p>When we use the byte-pair encoding algorithm, we break up the sequence into familiar chunks, if familiar chunks exist. We can this for the word <em>suspecte</em> which means <em>suspicious</em>. It breaks up this word into 2 tokens, since the word <em>suspecte</em> by itself wasn’t common enough in the tokenizer training set.</p> <p>In code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_tokenizer</span><span class="p">.</span><span class="n">processor</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">suspecte</span><span class="sh">"</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="sh">'</span><span class="s">▁suspec</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">te</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>These 2 tokens were common enugh in the tokenizer training set to be included as tokens.</p> <p>The underscore in “▁suspec” seems strange at first; sentencepiece encodes “word boundaries” or starts of words with “_”.</p> <p>To my surprise, I didn’t find any examples of <code class="language-plaintext highlighter-rouge">byte_fallback</code> being activated in the test set. Meaning that every word can be broken up into chunks that are known vocabulary tokens.</p> <p>Let’s take a sequence with a rather rare looking word “Ikhrata”, which is someone’s surname, from the test set:</p> <blockquote> <p>“It is not a matter of something we might choose to do,” said Hasan Ikhrata, executive director of the Southern California Assn. of Governments, which is planning for the state to start tracking miles driven by every California motorist by 2025.</p> </blockquote> <p>This word is not in the training set. How does the tokenizer handle this?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">source_tokenizer</span><span class="p">.</span><span class="n">processor</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="s">Ikhrata</span><span class="sh">"</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">[</span><span class="sh">'</span><span class="s">▁I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">kh</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ra</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">ta</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div> <p>It breaks up the word into familiar chunks, known vocabulary tokens. This is the advantage of using a tokenization algorithm like sentencepiece, compared to just breaking up sequences into words. The word “Ikhrata” would be unknown and we would not know what to do with it.</p> <p>What is the model’s output?</p> <blockquote> <p>« Il ne s’agit pas d’une chose que nous pourrions choisir de faire », a déclaré Hasan Ikherata, directeur exécutif de la Southern California Assn. of Governments, qui prévoit que l’État commencera à suivre les kilomètres parcourus par tous les automobilistes de la Californie d’ici 2025.</p> </blockquote> <p>“Ikhrata” was not found in the training set, and the model output “Ikherata”, which is not good.</p> <p>However, this shows that we can output something rather than <code class="language-plaintext highlighter-rouge">&lt;UNK&gt;</code>.</p> <h1 id="conclusion">Conclusion</h1> <p>We implemented an RNN model without attention similar to <d-cite key="10.5555/2969033.2969173"></d-cite> and a model with attention similar to <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> and found that the model with attention has better overall performance than the model without attention as evaluated by the BLEU score. However, <d-cite key="DBLP:journals/corr/BahdanauCB14"></d-cite> reports a steep decrease in BLEU score as the sentence length increases, which we were not able to reproduce. Our results are in line with <d-cite key="10.5555/2969033.2969173"></d-cite> which reported that the model without attention is robust to sentence length, but the paper that introduced attention is the very famous paper while the model without attention that reported robustness to sentence length is not nearly as cited.</p> <p>Attention forms the backbone of SOTA “language models” such as the GPT series, and as such became much more influential.</p> <p>Also of note that at the time, non-deep-learning based methods such as Moses, showed better performance on the translation task, but clearly the community invested more effort in deep learning based methods. The transformer model paid off and is significantly more influential than just in doing translation, which Moses is only capable of doing.</p> <p>Next, we’ll build on language models and implement a transformer model rather than an RNN model.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-10-03-sequence_to_sequence_translation.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Adam Amster. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-",title:"",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-about",title:"about",description:"",section:"Navigation",handler:()=>{window.location.href="/about"}},{id:"post-sequence-to-sequence-translation-part-1",title:"Sequence to sequence translation (part 1)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/sequence_to_sequence_models_1/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61.%61%6D%73%74%65%72%31@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>